{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b597e14",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "10979961",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Deprecation warnings have been disabled. Set TF_ENABLE_DEPRECATION_WARNINGS=1 to re-enable them.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-05-07 07:10:00.534886: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcudart.so.11.0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /tmp/ipykernel_478/3908576244.py:2: The name tf.enable_eager_execution is deprecated. Please use tf.compat.v1.enable_eager_execution instead.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "tf.enable_eager_execution()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e8d1d7fe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files train.txt and test.txt generated successfully.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import random\n",
    "\n",
    "# Function to load images and corresponding labels\n",
    "def load_images_and_labels(folder_path):\n",
    "    images = []\n",
    "    labels = []\n",
    "    label_names = []\n",
    "\n",
    "    # Iterate over folders (labels)\n",
    "    for label_folder in os.listdir(folder_path):\n",
    "        label_path = os.path.join(folder_path, label_folder)\n",
    "\n",
    "        # Skip if it's not a directory\n",
    "        if not os.path.isdir(label_path):\n",
    "            continue\n",
    "\n",
    "        # Append label name to the list\n",
    "        label_names.append(label_folder)\n",
    "\n",
    "        # Load images from the label folder\n",
    "        for image_file in os.listdir(label_path):\n",
    "            image_path = os.path.join(label_path, image_file)\n",
    "            # Append image path to the list\n",
    "            images.append(image_path)\n",
    "            # Append label index to the labels list\n",
    "            labels.append(len(label_names) - 1)  # Index of label name\n",
    "\n",
    "    return images, labels, label_names\n",
    "\n",
    "# Load images and labels\n",
    "vggface2_folder = \"Downloads/val\"\n",
    "vggface2_folder1=\"Downloads/test\"\n",
    "images, labels, label_names = load_images_and_labels(vggface2_folder)\n",
    "images1,labels1,label_names1=load_images_and_labels(vggface2_folder1)\n",
    "images=images+images1\n",
    "labels=labels+labels1\n",
    "label_names=label_names1\n",
    "# Split data into train and test sets (adjust the split ratio as needed)\n",
    "split_ratio = 0.95\n",
    "num_images = len(images)\n",
    "num_train = int(num_images * split_ratio)\n",
    "random.seed(42)\n",
    "train_indices = random.sample(range(num_images), num_train)\n",
    "\n",
    "test_indices = list(set(range(num_images)) - set(train_indices))\n",
    "\n",
    "# Write image paths and labels into train.txt and test.txt files\n",
    "def write_to_file(file_path, image_paths, image_labels):\n",
    "    with open(file_path, 'w') as file:\n",
    "        for path, label in zip(image_paths, image_labels):\n",
    "            file.write(f\"{path} {label}\\n\")\n",
    "\n",
    "write_to_file(\"Downloads/train.txt\", [images[i] for i in train_indices], [labels[i] for i in train_indices])\n",
    "write_to_file(\"Downloads/test.txt\", [images[i] for i in test_indices], [labels[i] for i in test_indices])\n",
    "\n",
    "print(\"Files train.txt and test.txt generated successfully.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e4a05bf5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "187808\n",
      "1468\n",
      "9885\n"
     ]
    }
   ],
   "source": [
    "\n",
    "weight_decay = 1e-4\n",
    "epoch_num = 300\n",
    "hash_bit = 512\n",
    "num_class = len(set(labels))\n",
    "input_shape = (224, 224, 3)\n",
    "label_smoothing = 0.1\n",
    "\n",
    "# training config\n",
    "batch_size = 128\n",
    "train_num = len(train_indices) #6149\n",
    "print(train_num)\n",
    "iterations_per_epoch = int(train_num / batch_size) + 1\n",
    "warm_iterations = iterations_per_epoch\n",
    "print(iterations_per_epoch)\n",
    "\n",
    "\n",
    "initial_learning_rate = 0.05\n",
    "minimum_learning_rate = 0.0001\n",
    "\n",
    "# test config\n",
    "test_batch_size = 128\n",
    "test_num = len(test_indices)\n",
    "print(test_num)\n",
    "test_iterations = int(test_num / test_batch_size) + 1\n",
    "\n",
    "log_file = 'result/vggface2/densenet-512-4.18.txt'\n",
    "\n",
    "# lr\n",
    "learning_rate = [0.1, 0.01, 0.001]\n",
    "boundaries = [80 * iterations_per_epoch, 120 * iterations_per_epoch]\n",
    "\n",
    "\n",
    "# path\n",
    "train_data_path = ''\n",
    "test_data_path = ''\n",
    "imagenet_train_path = 'Downloads/train.txt'\n",
    "imagenet_test_path = 'Downloads/test.txt'\n",
    "\n",
    "flowers17_train_path = \"./flowers17/train.txt\"\n",
    "flowers17_test_path = \"./flowers17/test.txt\"\n",
    "\n",
    "flowers102_train_path = \"./oxford-102-flowers/train.txt\"\n",
    "flowers102_test_path = \"./oxford-102-flowers/test.txt\"\n",
    "\n",
    "sun397_train_path = \"../dataset/imagenet/train_label.txt\"\n",
    "sun397_test_path = \"../dataset/imagenet/validation_label.txt\"\n",
    "\n",
    "short_side_scale = (256, 384)\n",
    "aspect_ratio_scale = (0.8, 1.25)\n",
    "hue_delta = (-36, 36)\n",
    "saturation_scale = (0.6, 1.4)\n",
    "brightness_scale = (0.6, 1.4)\n",
    "pca_std = 0.1\n",
    "\n",
    "mean = [103.939, 116.779, 123.68]\n",
    "std = [58.393, 57.12, 57.375]\n",
    "eigval = [55.46, 4.794, 1.148]\n",
    "eigvec = [[-0.5836, -0.6948, 0.4203],\n",
    "          [-0.5808, -0.0045, -0.8140],\n",
    "          [-0.5675, 0.7192, 0.4009]]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "65f8598c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "tf.enable_eager_execution()\n",
    "from tensorflow.keras.layers import Conv2D, BatchNormalization, Activation, MaxPool2D, Dropout, Flatten, Dense\n",
    "from tensorflow.keras import Model, models, Input, regularizers\n",
    "from tensorflow.keras.applications import VGG19, ResNet50, InceptionV3, DenseNet121, VGG16\n",
    "\n",
    "\n",
    "\n",
    "class VGG16Net(Model):\n",
    "    def __init__(self):\n",
    "        super(VGG16Net, self).__init__()\n",
    "        self.base_model = VGG16(weights='imagenet', include_top=False, pooling='max')\n",
    "        # self.base_model.trainable = False\n",
    "        for i in range(len(self.base_model.layers) - 2):  # print(len(model.layers))=23\n",
    "            self.base_model.layers[i].trainable = False\n",
    "        self.d1 = Dense(units=hash_bit, activation=tf.nn.sigmoid)\n",
    "        self.d2 = Dense(units=num_class, activation=tf.nn.softmax)\n",
    "\n",
    "    def call(self, x):\n",
    "        x = self.base_model(x)\n",
    "        hash_code = self.d1(x)\n",
    "        prediction = self.d2(hash_code)\n",
    "        return prediction, hash_code\n",
    "\n",
    "class VGG19Net(Model):\n",
    "    def __init__(self):\n",
    "        super(VGG19Net, self).__init__()\n",
    "        self.base_model = VGG19(weights='imagenet', include_top=False, pooling='max')\n",
    "        # self.base_model.trainable = False\n",
    "        #for i in range(len(self.base_model.layers)):\n",
    "        #    self.base_model.layers[i].trainable = False\n",
    "        self.d1 = Dense(units=hash_bit, activation=None)\n",
    "        self.bn1 = BatchNormalization()\n",
    "        self.ac1 = tf.keras.layers.Activation(\"sigmoid\")\n",
    "        self.d2 = Dense(units=num_class, activation=tf.nn.softmax)\n",
    "\n",
    "    def call(self, x):\n",
    "        x = self.base_model(x)\n",
    "        hash_code = self.d1(x)\n",
    "        hash_code = self.bn1(hash_code)\n",
    "        hash_code = self.ac1(hash_code)\n",
    "        prediction = self.d2(hash_code)\n",
    "        return prediction, hash_code    \n",
    "    \n",
    "\n",
    "\n",
    "class VGG19Net(Model):\n",
    "    def __init__(self):\n",
    "        super(VGG19Net, self).__init__()\n",
    "        self.base_model = VGG19(weights='imagenet', include_top=False, pooling='max')\n",
    "        # self.base_model.trainable = False\n",
    "        #for i in range(len(self.base_model.layers)):\n",
    "        #    self.base_model.layers[i].trainable = False\n",
    "        self.d1 = Dense(units=hash_bit, activation=None)\n",
    "        self.bn1 = BatchNormalization()\n",
    "        self.ac1 = tf.keras.layers.Activation(\"sigmoid\")\n",
    "        self.d2 = Dense(units=num_class, activation=tf.nn.softmax)\n",
    "\n",
    "    def call(self, x):\n",
    "        x = self.base_model(x)\n",
    "        hash_code = self.d1(x)\n",
    "        hash_code = self.bn1(hash_code)\n",
    "        hash_code = self.ac1(hash_code)\n",
    "        prediction = self.d2(hash_code)\n",
    "        return prediction, hash_code    \n",
    "\n",
    "\n",
    "\n",
    "def Resnet_Model():\n",
    "    base_model = ResNet50(weights='imagenet', include_top=False, layers=tf.keras.layers, input_shape=(224, 224, 3))\n",
    "    print(len(base_model.layers))  # 175\n",
    "    #for layer in base_model.layers[:170]:\n",
    "    #    layer.trainable = False\n",
    "    #for layer in base_model.layers[170:]:\n",
    "    #    layer.trainable = True\n",
    "    x = base_model.output\n",
    "    x = tf.keras.layers.GlobalAveragePooling2D(name='average_pool')(x)\n",
    "    x = tf.keras.layers.Flatten(name='flatten')(x)\n",
    "    hash_value = tf.keras.layers.Dense(units=hash_bit, activation=None, kernel_regularizer=regularizers.l2(0.001))(x)\n",
    "    # hash_value = tf.keras.layers.Dense(units=hash_bit, activation=None)(x)\n",
    "    hash_value = tf.keras.layers.BatchNormalization()(hash_value)\n",
    "    hash_value = tf.keras.layers.Activation(\"sigmoid\")(hash_value)\n",
    "    prediction = tf.keras.layers.Dense(units=num_class, activation=\"softmax\",\n",
    "                                       kernel_regularizer=regularizers.l2(0.001))(hash_value)\n",
    "    # prediction = tf.keras.layers.Dense(units=num_class, activation=\"softmax\")(hash_value)\n",
    "    model = models.Model(inputs=base_model.input, outputs=[prediction, hash_value])\n",
    "    return model\n",
    "\n",
    "\n",
    "def DenseNet_Model():\n",
    "    base_model = DenseNet121(weights='imagenet', include_top=False, input_shape=(224, 224, 3))\n",
    "    base_model.trainable = False\n",
    "    print(len(base_model.layers))  # 427\n",
    "    for layer in base_model.layers[:415]:\n",
    "       layer.trainable = False\n",
    "    for layer in base_model.layers[415:]:\n",
    "       layer.trainable = True\n",
    "    x = base_model.output\n",
    "    x = tf.keras.layers.GlobalAveragePooling2D(name='average_pool')(x)\n",
    "    x = tf.keras.layers.Flatten(name='flatten')(x)\n",
    "    #x = tf.keras.layers.Dropout(0.5)(x)\n",
    "    hash_value = tf.keras.layers.Dense(units=hash_bit, activation=None)(x)\n",
    "    hash_value = tf.keras.layers.BatchNormalization()(hash_value)\n",
    "    hash_value = tf.keras.layers.Activation(\"sigmoid\")(hash_value)\n",
    "    prediction = tf.keras.layers.Dense(units=num_class, activation=\"softmax\")(hash_value)\n",
    "    model = models.Model(inputs=base_model.input, outputs=[prediction, hash_value])\n",
    "    return model\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "bb4f28ad",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting opencv-python\n",
      "  Downloading opencv_python-4.9.0.80-cp37-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (62.2 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m62.2/62.2 MB\u001b[0m \u001b[31m1.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: numpy>=1.17.0 in /usr/local/lib/python3.8/dist-packages (from opencv-python) (1.21.1)\n",
      "Installing collected packages: opencv-python\n",
      "Successfully installed opencv-python-4.9.0.80\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0m--- Logging error ---\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/local/lib/python3.8/dist-packages/pip/_internal/utils/logging.py\", line 177, in emit\n",
      "    self.console.print(renderable, overflow=\"ignore\", crop=False, style=style)\n",
      "  File \"/usr/local/lib/python3.8/dist-packages/pip/_vendor/rich/console.py\", line 1673, in print\n",
      "    extend(render(renderable, render_options))\n",
      "  File \"/usr/local/lib/python3.8/dist-packages/pip/_vendor/rich/console.py\", line 1305, in render\n",
      "    for render_output in iter_render:\n",
      "  File \"/usr/local/lib/python3.8/dist-packages/pip/_internal/utils/logging.py\", line 134, in __rich_console__\n",
      "    for line in lines:\n",
      "  File \"/usr/local/lib/python3.8/dist-packages/pip/_vendor/rich/segment.py\", line 249, in split_lines\n",
      "    for segment in segments:\n",
      "  File \"/usr/local/lib/python3.8/dist-packages/pip/_vendor/rich/console.py\", line 1283, in render\n",
      "    renderable = rich_cast(renderable)\n",
      "  File \"/usr/local/lib/python3.8/dist-packages/pip/_vendor/rich/protocol.py\", line 36, in rich_cast\n",
      "    renderable = cast_method()\n",
      "  File \"/usr/local/lib/python3.8/dist-packages/pip/_internal/self_outdated_check.py\", line 130, in __rich__\n",
      "    pip_cmd = get_best_invocation_for_this_pip()\n",
      "  File \"/usr/local/lib/python3.8/dist-packages/pip/_internal/utils/entrypoints.py\", line 58, in get_best_invocation_for_this_pip\n",
      "    if found_executable and os.path.samefile(\n",
      "  File \"/usr/lib/python3.8/genericpath.py\", line 101, in samefile\n",
      "    s2 = os.stat(f2)\n",
      "FileNotFoundError: [Errno 2] No such file or directory: '/usr/bin/pip'\n",
      "Call stack:\n",
      "  File \"/usr/local/bin/pip\", line 8, in <module>\n",
      "    sys.exit(main())\n",
      "  File \"/usr/local/lib/python3.8/dist-packages/pip/_internal/cli/main.py\", line 70, in main\n",
      "    return command.main(cmd_args)\n",
      "  File \"/usr/local/lib/python3.8/dist-packages/pip/_internal/cli/base_command.py\", line 101, in main\n",
      "    return self._main(args)\n",
      "  File \"/usr/local/lib/python3.8/dist-packages/pip/_internal/cli/base_command.py\", line 223, in _main\n",
      "    self.handle_pip_version_check(options)\n",
      "  File \"/usr/local/lib/python3.8/dist-packages/pip/_internal/cli/req_command.py\", line 190, in handle_pip_version_check\n",
      "    pip_self_version_check(session, options)\n",
      "  File \"/usr/local/lib/python3.8/dist-packages/pip/_internal/self_outdated_check.py\", line 236, in pip_self_version_check\n",
      "    logger.warning(\"[present-rich] %s\", upgrade_prompt)\n",
      "  File \"/usr/lib/python3.8/logging/__init__.py\", line 1458, in warning\n",
      "    self._log(WARNING, msg, args, **kwargs)\n",
      "  File \"/usr/lib/python3.8/logging/__init__.py\", line 1589, in _log\n",
      "    self.handle(record)\n",
      "  File \"/usr/lib/python3.8/logging/__init__.py\", line 1599, in handle\n",
      "    self.callHandlers(record)\n",
      "  File \"/usr/lib/python3.8/logging/__init__.py\", line 1661, in callHandlers\n",
      "    hdlr.handle(record)\n",
      "  File \"/usr/lib/python3.8/logging/__init__.py\", line 954, in handle\n",
      "    self.emit(record)\n",
      "  File \"/usr/local/lib/python3.8/dist-packages/pip/_internal/utils/logging.py\", line 179, in emit\n",
      "    self.handleError(record)\n",
      "Message: '[present-rich] %s'\n",
      "Arguments: (UpgradePrompt(old='22.2.2', new='24.0'),)\n"
     ]
    }
   ],
   "source": [
    "# @title Default title text\n",
    "#!/usr/bin/python3\n",
    "#coding=utf-8\n",
    "#coding=gbk\n",
    "!pip install opencv-python\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import cv2\n",
    "\n",
    "\n",
    "def load_list(list_path):\n",
    "    images = []\n",
    "    labels = []\n",
    "    print(list_path)\n",
    "    with open(list_path, 'r') as f:\n",
    "        for line in f:\n",
    "            lines = line.replace('\\n', '').split('\\t')\n",
    "            if len(lines) == 1:\n",
    "                lines = line.replace('\\n', '').split()\n",
    "            # print(lines)\n",
    "            # images.append(os.path.join(image_root_path, line[0]))\n",
    "            images.append(lines[0])\n",
    "            labels.append(int(lines[1]))\n",
    "    return images, labels\n",
    "\n",
    "\n",
    "def random_size(image, target_size=None):\n",
    "    height, width, _ = np.shape(image)\n",
    "    if height < width:\n",
    "        size_ratio = target_size / height\n",
    "    else:\n",
    "        size_ratio = target_size / width\n",
    "    resize_shape = (int(width * size_ratio), int(height * size_ratio))\n",
    "    return cv2.resize(image, resize_shape)\n",
    "\n",
    "\n",
    "def normalize(image):\n",
    "    # iamgenet归一化中用到的数�?\n",
    "    mean = [103.939, 116.779, 123.68]\n",
    "    std = [58.393, 57.12, 57.375]\n",
    "    for i in range(3):\n",
    "        image[..., i] = (image[..., i] - mean[i]) / std[i]\n",
    "    return image\n",
    "\n",
    "\n",
    "def random_crop(image):\n",
    "    height, width, _ = np.shape(image)\n",
    "    if height < width:\n",
    "        num = width - height\n",
    "        import random\n",
    "        value = random.randint(0, num)\n",
    "        value //= 2\n",
    "        image = image[:, value:224 + value]\n",
    "        image = cv2.resize(image, (224, 224))\n",
    "\n",
    "    else:\n",
    "        num = height - width\n",
    "        import random\n",
    "        value = random.randint(0, num)\n",
    "        value //= 2\n",
    "        image = image[value:224 + value, :]\n",
    "        image = cv2.resize(image, (224, 224))\n",
    "    return image\n",
    "\n",
    "#�������ǵ�jpg�ļ����н��룬�����ά����\n",
    "def load_preprosess_image(path,label, category_num):\n",
    "    #��ȡ·��\n",
    "    image = cv2.imread(path.numpy().decode()).astype(np.float32)\n",
    "    #����\n",
    "    #image=tf.image.decode_jpeg(image,channels=3)#��ɫͼ��Ϊ3��channel\n",
    "    #��ͼ��ı�Ϊͬ���Ĵ�С�����òü�����Ť��,����Ӧ����Ť��\n",
    "    image=tf.image.resize(image,[256,256])\n",
    "    #����ü�ͼ��\n",
    "    image=tf.image.random_crop(image,[224,224,3])\n",
    "    #������·�תͼ��\n",
    "    image=tf.image.random_flip_left_right(image)\n",
    "    #������·�ת\n",
    "    image=tf.image.random_flip_up_down(image)\n",
    "    #����ı�ͼ�������\n",
    "    image=tf.image.random_brightness(image,0.5)\n",
    "    #����ı�Աȶ�\n",
    "    image=tf.image.random_contrast(image,0,1)\n",
    "    #�ı���������\n",
    "    image=tf.cast(image,tf.float32)\n",
    "    #��ͼ����й�һ��\n",
    "    image=image/255\n",
    "    #���ڻ���Ҫ��label���д��������������б�[1,2,3],\n",
    "    #��Ҫ���[[1].[2].[3]]\n",
    "    label_one_hot = np.zeros(category_num)\n",
    "    label_one_hot[label] = 1.0\n",
    "    return image, label_one_hot\n",
    "\n",
    "\n",
    "def load_image(image_path, label, category_num):\n",
    "    #print(image_path)\n",
    "    #image_path = \"/home/ubuntu/shy/tf2_learn/21_pokemon/\" + image_path\n",
    "    #print(image_path)\n",
    "    image = cv2.imread(image_path.numpy().decode()).astype(np.float32)\n",
    "    #print(image.shape)\n",
    "    image = random_size(image, target_size=256)\n",
    "    image = random_crop(image)\n",
    "    image = normalize(image)\n",
    "    label_one_hot = np.zeros(category_num)\n",
    "    label_one_hot[label] = 1.0\n",
    "\n",
    "    return image, label_one_hot\n",
    "\n",
    "\n",
    "def train_iterator(train_list_path, class_nums, batch_size):\n",
    "    # train_data_path路径TXT文件\n",
    "    # images, labels = load_list(train_data_path) # jfkdsjf.jpg 2\n",
    "    images, labels = load_list(train_list_path)  # jfkdsjf.jpg 2\n",
    "    print(images)\n",
    "    #img = []\n",
    "    #for image in images:\n",
    "    #  image = \"/home/ubuntu/shy/tf2_learn/21_pokemon/\" + image\n",
    "     # img.append(image)\n",
    "    # print(images)\n",
    "    print(len(images), len(labels))\n",
    "    dataset = tf.data.Dataset.from_tensor_slices((images, labels))\n",
    "    dataset = dataset.map(\n",
    "        lambda x, y: tf.py_function(load_image, inp=[x, y, class_nums], Tout=[tf.float32, tf.float32]),\n",
    "        num_parallel_calls=tf.data.experimental.AUTOTUNE)\n",
    "    #dataset = dataset.shuffle(len(images))  # 测试集不需�?\n",
    "    dataset = dataset.repeat()  # 测试集不需�?\n",
    "    dataset = dataset.batch(batch_size)\n",
    "    it = dataset.__iter__()\n",
    "    #return it, images\n",
    "    return it\n",
    "\n",
    "def test_iterator(test_list_path, class_nums, batch_size):\n",
    "    images, labels = load_list(test_list_path)  # jfkdsjf.jpg 2\n",
    "    print(len(images))\n",
    "    print(len(labels))\n",
    "    #print(images, labels)\n",
    "    dataset = tf.data.Dataset.from_tensor_slices((images, labels))\n",
    "    dataset = dataset.map(\n",
    "        lambda x, y: tf.py_function(load_image, inp=[x, y, class_nums], Tout=[tf.float32, tf.float32]),\n",
    "        num_parallel_calls=tf.data.experimental.AUTOTUNE)\n",
    "    dataset = dataset.batch(batch_size)\n",
    "    it = dataset.__iter__()\n",
    "    return it, images\n",
    "    #print(images)\n",
    "    return it\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "6d3bdf18",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-05-05 17:33:03.604246: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcuda.so.1\n",
      "2024-05-05 17:33:03.647623: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1666] Found device 0 with properties: \n",
      "name: NVIDIA A100 80GB PCIe major: 8 minor: 0 memoryClockRate(GHz): 1.41\n",
      "pciBusID: 0000:4b:00.0\n",
      "2024-05-05 17:33:03.647684: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcudart.so.11.0\n",
      "2024-05-05 17:33:03.729824: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcublas.so.11\n",
      "2024-05-05 17:33:03.798486: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcufft.so.10\n",
      "2024-05-05 17:33:03.816270: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcurand.so.10\n",
      "2024-05-05 17:33:03.831255: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcusolver.so.11\n",
      "2024-05-05 17:33:03.845211: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcusparse.so.11\n",
      "2024-05-05 17:33:03.845453: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcudnn.so.8\n",
      "2024-05-05 17:33:03.847976: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1794] Adding visible gpu devices: 0\n",
      "2024-05-05 17:33:03.918948: I tensorflow/core/platform/profile_utils/cpu_utils.cc:109] CPU Frequency: 2100000000 Hz\n",
      "2024-05-05 17:33:03.922074: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x4130530 initialized for platform Host (this does not guarantee that XLA will be used). Devices:\n",
      "2024-05-05 17:33:03.922107: I tensorflow/compiler/xla/service/service.cc:176]   StreamExecutor device (0): Host, Default Version\n",
      "2024-05-05 17:33:04.180462: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x178d590 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:\n",
      "2024-05-05 17:33:04.180505: I tensorflow/compiler/xla/service/service.cc:176]   StreamExecutor device (0): NVIDIA A100 80GB PCIe, Compute Capability 8.0\n",
      "2024-05-05 17:33:04.181974: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1666] Found device 0 with properties: \n",
      "name: NVIDIA A100 80GB PCIe major: 8 minor: 0 memoryClockRate(GHz): 1.41\n",
      "pciBusID: 0000:4b:00.0\n",
      "2024-05-05 17:33:04.182031: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcudart.so.11.0\n",
      "2024-05-05 17:33:04.182069: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcublas.so.11\n",
      "2024-05-05 17:33:04.182085: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcufft.so.10\n",
      "2024-05-05 17:33:04.182099: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcurand.so.10\n",
      "2024-05-05 17:33:04.182114: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcusolver.so.11\n",
      "2024-05-05 17:33:04.182128: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcusparse.so.11\n",
      "2024-05-05 17:33:04.182141: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcudnn.so.8\n",
      "2024-05-05 17:33:04.184856: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1794] Adding visible gpu devices: 0\n",
      "2024-05-05 17:33:04.184912: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcudart.so.11.0\n",
      "2024-05-05 17:33:08.539036: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1206] Device interconnect StreamExecutor with strength 1 edge matrix:\n",
      "2024-05-05 17:33:08.539092: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1212]      0 \n",
      "2024-05-05 17:33:08.539103: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1225] 0:   N \n",
      "2024-05-05 17:33:08.542759: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1351] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 78026 MB memory) -> physical GPU (device: 0, name: NVIDIA A100 80GB PCIe, pci bus id: 0000:4b:00.0, compute capability: 8.0)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading data from https://github.com/keras-team/keras-applications/releases/download/resnet/resnet50_weights_tf_dim_ordering_tf_kernels_notop.h5\n",
      "94773248/94765736 [==============================] - 9s 0us/step\n",
      "175\n",
      "-------------load the model-----------------\n",
      "succesful\n",
      "loaded the model b200834cs/resnet/model.ckpt-4\n",
      "Downloads/train.txt\n",
      "187808\n",
      "187808\n",
      "WARNING:tensorflow:TensorFlow will not use sklearn by default. This improves performance in some cases. To enable sklearn export the environment variable  TF_ALLOW_IOLIBS=1.\n",
      "WARNING:tensorflow:TensorFlow will not use Dask by default. This improves performance in some cases. To enable Dask export the environment variable  TF_ALLOW_IOLIBS=1.\n",
      "WARNING:tensorflow:TensorFlow will not use Pandas by default. This improves performance in some cases. To enable Pandas export the environment variable  TF_ALLOW_IOLIBS=1.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|                                                                                                | 0/1468 [00:00<?, ?it/s]2024-05-05 17:33:35.676688: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcublas.so.11\n",
      "2024-05-05 17:33:39.621159: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcudnn.so.8\n",
      "100%|█████████████████████████████████████████████████████████████████████████████████████| 1468/1468 [35:55<00:00,  1.47s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test, loss:0.216974, accuracy:0.949804\n",
      "[1. 1. 0. ... 1. 0. 0.]\n",
      "[0. 0. 0. ... 0. 0. 0.]\n",
      "Downloads/test.txt\n",
      "9885\n",
      "9885\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████████████████████████████████████| 78/78 [01:44<00:00,  1.34s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test, loss:0.369534, accuracy:0.909272\n",
      "[0. 0. 0. ... 0. 0. 1.]\n",
      "[0. 0. 0. ... 0. 0. 0.]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "#!/usr/bin/python?\n",
    "# -*- coding: utf-8 -*-import tensorflow as tf\n",
    "#load load_file config_file\n",
    "\n",
    "import numpy as np\n",
    "import pickle as p\n",
    "from tqdm import tqdm\n",
    "import os\n",
    "import cv2\n",
    "from tensorflow.keras import models, optimizers, Sequential\n",
    "from tensorflow.keras.layers import Input\n",
    "\n",
    "import tensorflow as tf\n",
    "import math\n",
    "\n",
    "\n",
    "def getBinaryTensor(imgTensor, boundary=0.5):\n",
    "    one = tf.ones_like(imgTensor)\n",
    "    zero = tf.zeros_like(imgTensor)\n",
    "    return tf.where(imgTensor > boundary, one, zero)\n",
    "\n",
    "\n",
    "def hash_loss_fn(hash_input):\n",
    "    loss1 = -1 * tf.reduce_mean(tf.square(hash_input - 0.5)) + 0.25  # ���ֵΪ0.25\n",
    "    loss2 = tf.reduce_mean(tf.square(tf.reduce_mean(hash_input, axis=1) - 0.5))\n",
    "    return loss1 + loss2\n",
    "\n",
    "\n",
    "def cross_entropy(y_true, y_pred):\n",
    "    cross_entropy = tf.keras.losses.categorical_crossentropy(y_true, y_pred)\n",
    "    return tf.reduce_mean(cross_entropy)\n",
    "\n",
    "\n",
    "def l2_loss(model, weights=weight_decay):\n",
    "    variable_list = []\n",
    "    for v in model.trainable_variables:\n",
    "        if 'kernel' in v.name:\n",
    "            variable_list.append(tf.nn.l2_loss(v))\n",
    "    return tf.add_n(variable_list) * weights\n",
    "\n",
    "\n",
    "def accuracy(y_true, y_pred):\n",
    "    correct_num = tf.equal(tf.argmax(y_true, -1), tf.argmax(y_pred, -1))\n",
    "    accuracy = tf.reduce_mean(tf.cast(correct_num, dtype=tf.float32))\n",
    "    return accuracy\n",
    "\n",
    "\n",
    "@tf.function\n",
    "def test_step(model, x, y):\n",
    "    prediction, hash_value = model(x, training=False)  # �޸�\n",
    "    ce = cross_entropy(y, prediction)\n",
    "    return ce, prediction, hash_value\n",
    "\n",
    "\n",
    "def test(model, test_iter,xyz):\n",
    "    sum_loss = 0\n",
    "    sum_accuracy = 0\n",
    "    count = 0\n",
    "    hashh_train=np.array([])\n",
    "    labell_train=np.array([])\n",
    "    test_data_iterator, images = test_iterator(xyz, num_class, batch_size)\n",
    "    filewriter = open('hash_code_test_resnet.txt', 'w')\n",
    "    for i in tqdm(range(test_iter)):\n",
    "        x, y = test_data_iterator.next()\n",
    "        x = tf.cast(x, tf.float32)  # �����Լ��е�ͼ������float32\n",
    "        loss, prediction, hash_value = test_step(model, x, y)\n",
    "        sum_loss += loss\n",
    "        sum_accuracy += accuracy(y, prediction)\n",
    "        hash_value = getBinaryTensor(hash_value)\n",
    "        hashh_train=np.append(hashh_train,hash_value)\n",
    "        labell_train=np.append(labell_train,y)\n",
    "\n",
    "        for j in range(hash_value.shape[0]):\n",
    "            filewriter.writelines(  str(hash_value[j].numpy())  + \"\\n\")\n",
    "            count += 1\n",
    "    print('test, loss:%f, accuracy:%f' %\n",
    "          (sum_loss / test_iter, sum_accuracy / test_iter))\n",
    "    return hashh_train,labell_train\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    import os\n",
    "\n",
    "    # get model\n",
    "    #img_input = Input(shape=(224, 224, 3))\n",
    "    #net = VGG19Net()\n",
    "    #net.build((None,224,224,3))\n",
    "    #output = net(img_input)\n",
    "    #model = models.Model(img_input, output)\n",
    "    # os.environ['CUDA_VISIBLE_DEVICES'] = '1'\n",
    "    # physical_devices = tf.config.experimental.list_physical_devices(device_type='GPU')\n",
    "    # tf.config.experimental.set_memory_growth(device=physical_devices[0], enable=True)\n",
    "\n",
    "    model = Resnet_Model()\n",
    "    model.build((None, 224, 224, 3))\n",
    "\n",
    "    checkpoint_save_path = \"b200834cs/resnet/\"\n",
    "    checkpoint = tf.train.Checkpoint(myAwesomeModel=model)\n",
    "    print('-------------load the model-----------------')\n",
    "    if checkpoint.restore(tf.train.latest_checkpoint(checkpoint_save_path)):\n",
    "       print(\"succesful\")\n",
    "    print(\"loaded the model \" + checkpoint_save_path + \"model.ckpt-4\")\n",
    "    train_acc,traint = test(model, iterations_per_epoch,\"Downloads/train.txt\")\n",
    "    print(train_acc)\n",
    "    print(traint)\n",
    "    traint_binary_resnet=train_acc.reshape(int(len(train_acc)/512),512)\n",
    "    test_acc,testt = test(model, test_iterations,\"Downloads/test.txt\")\n",
    "    print(test_acc)\n",
    "    print(testt)\n",
    "    testt_binary_resnet=test_acc.reshape(int(len(test_acc)/512),512)\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "409dd6bd",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading data from https://github.com/keras-team/keras-applications/releases/download/densenet/densenet121_weights_tf_dim_ordering_tf_kernels_notop.h5\n",
      "29089792/29084464 [==============================] - 3s 0us/step\n",
      "427\n",
      "-------------load the model-----------------\n",
      "succesful\n",
      "loaded the model b200834cs/densenet/model.ckpt-4\n",
      "Downloads/train.txt\n",
      "187808\n",
      "187808\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████████████████████████████████| 1468/1468 [36:36<00:00,  1.50s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test, loss:3.143089, accuracy:0.302697\n",
      "[0. 0. 1. ... 0. 0. 1.]\n",
      "[0. 0. 0. ... 0. 0. 0.]\n",
      "Downloads/test.txt\n",
      "9885\n",
      "9885\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████████████████████████████████████| 77/77 [01:44<00:00,  1.36s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test, loss:3.226787, accuracy:0.283787\n",
      "[1. 0. 1. ... 1. 1. 1.]\n",
      "[0. 0. 0. ... 0. 0. 0.]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "#!/usr/bin/python?\n",
    "# -*- coding: utf-8 -*-import tensorflow as tf\n",
    "#load load_file config_file\n",
    "\n",
    "import numpy as np\n",
    "import pickle as p\n",
    "from tqdm import tqdm\n",
    "import os\n",
    "import cv2\n",
    "from tensorflow.keras import models, optimizers, Sequential\n",
    "from tensorflow.keras.layers import Input\n",
    "\n",
    "import tensorflow as tf\n",
    "import math\n",
    "\n",
    "\n",
    "def getBinaryTensor(imgTensor, boundary=0.5):\n",
    "    one = tf.ones_like(imgTensor)\n",
    "    zero = tf.zeros_like(imgTensor)\n",
    "    return tf.where(imgTensor > boundary, one, zero)\n",
    "\n",
    "\n",
    "def hash_loss_fn(hash_input):\n",
    "    loss1 = -1 * tf.reduce_mean(tf.square(hash_input - 0.5)) + 0.25  # ���ֵΪ0.25\n",
    "    loss2 = tf.reduce_mean(tf.square(tf.reduce_mean(hash_input, axis=1) - 0.5))\n",
    "    return loss1 + loss2\n",
    "\n",
    "\n",
    "def cross_entropy(y_true, y_pred):\n",
    "    cross_entropy = tf.keras.losses.categorical_crossentropy(y_true, y_pred)\n",
    "    return tf.reduce_mean(cross_entropy)\n",
    "\n",
    "\n",
    "def l2_loss(model, weights=weight_decay):\n",
    "    variable_list = []\n",
    "    for v in model.trainable_variables:\n",
    "        if 'kernel' in v.name:\n",
    "            variable_list.append(tf.nn.l2_loss(v))\n",
    "    return tf.add_n(variable_list) * weights\n",
    "\n",
    "\n",
    "def accuracy(y_true, y_pred):\n",
    "    correct_num = tf.equal(tf.argmax(y_true, -1), tf.argmax(y_pred, -1))\n",
    "    accuracy = tf.reduce_mean(tf.cast(correct_num, dtype=tf.float32))\n",
    "    return accuracy\n",
    "\n",
    "\n",
    "@tf.function\n",
    "def test_step(model, x, y):\n",
    "    prediction, hash_value = model(x, training=False)  # �޸�\n",
    "    ce = cross_entropy(y, prediction)\n",
    "    return ce, prediction, hash_value\n",
    "\n",
    "\n",
    "def test(model, test_iter,xyz):\n",
    "    sum_loss = 0\n",
    "    sum_accuracy = 0\n",
    "    count = 0\n",
    "    hashh_train=np.array([])\n",
    "    labell_train=np.array([])\n",
    "    test_data_iterator, images = test_iterator(xyz, num_class, batch_size)\n",
    "    filewriter = open('hash_code_test_resnet.txt', 'w')\n",
    "    for i in tqdm(range(test_iter)):\n",
    "        x, y = test_data_iterator.next()\n",
    "        x = tf.cast(x, tf.float32)  # �����Լ��е�ͼ������float32\n",
    "        loss, prediction, hash_value = test_step(model, x, y)\n",
    "        sum_loss += loss\n",
    "        sum_accuracy += accuracy(y, prediction)\n",
    "        hash_value = getBinaryTensor(hash_value)\n",
    "        hashh_train=np.append(hashh_train,hash_value)\n",
    "        labell_train=np.append(labell_train,y)\n",
    "\n",
    "        for j in range(hash_value.shape[0]):\n",
    "            filewriter.writelines(  str(hash_value[j].numpy())  + \"\\n\")\n",
    "            count += 1\n",
    "    print('test, loss:%f, accuracy:%f' %\n",
    "          (sum_loss / test_iter, sum_accuracy / test_iter))\n",
    "    return hashh_train,labell_train\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    import os\n",
    "\n",
    "    # get model\n",
    "    #img_input = Input(shape=(224, 224, 3))\n",
    "    #net = VGG19Net()\n",
    "    #net.build((None,224,224,3))\n",
    "    #output = net(img_input)\n",
    "    #model = models.Model(img_input, output)\n",
    "#     os.environ['CUDA_VISIBLE_DEVICES'] = '1'\n",
    "#     physical_devices = tf.config.experimental.list_physical_devices(device_type='GPU')\n",
    "#     tf.config.experimental.set_memory_growth(device=physical_devices[0], enable=True)\n",
    "\n",
    "    model = DenseNet_Model()\n",
    "    model.build((None, 224, 224, 3))\n",
    "\n",
    "    checkpoint_save_path = \"b200834cs/densenet/\"\n",
    "    checkpoint = tf.train.Checkpoint(myAwesomeModel=model)\n",
    "    print('-------------load the model-----------------')\n",
    "    if checkpoint.restore(tf.train.latest_checkpoint(checkpoint_save_path)):\n",
    "       print(\"succesful\")\n",
    "    print(\"loaded the model \" + checkpoint_save_path + \"model.ckpt-4\")\n",
    "    train_acc,traint = test(model, iterations_per_epoch,\"Downloads/train.txt\")\n",
    "    print(train_acc)\n",
    "    print(traint)\n",
    "    traint_binary_densenet=train_acc.reshape(int(len(train_acc)/512),512)\n",
    "    test_acc,testt = test(model, test_iterations-1,\"Downloads/test.txt\")\n",
    "    print(test_acc)\n",
    "    print(testt)\n",
    "    testt_binary_densenet=test_acc.reshape(int(len(test_acc)/512),512)\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "df4c0b40",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading data from https://github.com/fchollet/deep-learning-models/releases/download/v0.1/vgg19_weights_tf_dim_ordering_tf_kernels_notop.h5\n",
      "80142336/80134624 [==============================] - 8s 0us/step\n",
      "-------------load the model-----------------\n",
      "succesful\n",
      "loaded the model b200834cs/vgg/model.ckpt-4\n",
      "Downloads/train.txt\n",
      "187808\n",
      "187808\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████████████████████████████████| 1468/1468 [36:46<00:00,  1.50s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test, loss:0.293885, accuracy:0.934717\n",
      "[0. 1. 0. ... 1. 1. 0.]\n",
      "[0. 0. 0. ... 0. 0. 0.]\n",
      "Downloads/test.txt\n",
      "9885\n",
      "9885\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████████████████████████████████████| 78/78 [01:47<00:00,  1.38s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test, loss:0.447672, accuracy:0.896227\n",
      "[0. 1. 1. ... 1. 0. 1.]\n",
      "[0. 0. 0. ... 0. 0. 0.]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "#!/usr/bin/python?\n",
    "# -*- coding: utf-8 -*-import tensorflow as tf\n",
    "#load load_file config_file\n",
    "\n",
    "import numpy as np\n",
    "import pickle as p\n",
    "from tqdm import tqdm\n",
    "import os\n",
    "import cv2\n",
    "from tensorflow.keras import models, optimizers, Sequential\n",
    "from tensorflow.keras.layers import Input\n",
    "\n",
    "import tensorflow as tf\n",
    "import math\n",
    "\n",
    "\n",
    "def getBinaryTensor(imgTensor, boundary=0.5):\n",
    "    one = tf.ones_like(imgTensor)\n",
    "    zero = tf.zeros_like(imgTensor)\n",
    "    return tf.where(imgTensor > boundary, one, zero)\n",
    "\n",
    "\n",
    "def hash_loss_fn(hash_input):\n",
    "    loss1 = -1 * tf.reduce_mean(tf.square(hash_input - 0.5)) + 0.25  # ���ֵΪ0.25\n",
    "    loss2 = tf.reduce_mean(tf.square(tf.reduce_mean(hash_input, axis=1) - 0.5))\n",
    "    return loss1 + loss2\n",
    "\n",
    "\n",
    "def cross_entropy(y_true, y_pred):\n",
    "    cross_entropy = tf.keras.losses.categorical_crossentropy(y_true, y_pred)\n",
    "    return tf.reduce_mean(cross_entropy)\n",
    "\n",
    "\n",
    "def l2_loss(model, weights=weight_decay):\n",
    "    variable_list = []\n",
    "    for v in model.trainable_variables:\n",
    "        if 'kernel' in v.name:\n",
    "            variable_list.append(tf.nn.l2_loss(v))\n",
    "    return tf.add_n(variable_list) * weights\n",
    "\n",
    "\n",
    "def accuracy(y_true, y_pred):\n",
    "    correct_num = tf.equal(tf.argmax(y_true, -1), tf.argmax(y_pred, -1))\n",
    "    accuracy = tf.reduce_mean(tf.cast(correct_num, dtype=tf.float32))\n",
    "    return accuracy\n",
    "\n",
    "\n",
    "@tf.function\n",
    "def test_step(model, x, y):\n",
    "    prediction, hash_value = model(x, training=False)  # �޸�\n",
    "    ce = cross_entropy(y, prediction)\n",
    "    return ce, prediction, hash_value\n",
    "\n",
    "\n",
    "def test(model, test_iter,xyz):\n",
    "    sum_loss = 0\n",
    "    sum_accuracy = 0\n",
    "    count = 0\n",
    "    hashh_train=np.array([])\n",
    "    labell_train=np.array([])\n",
    "    test_data_iterator, images = test_iterator(xyz, num_class, batch_size)\n",
    "    filewriter = open('hash_code_test_vgg19.txt', 'w')\n",
    "    for i in tqdm(range(test_iter)):\n",
    "        x, y = test_data_iterator.next()\n",
    "        x = tf.cast(x, tf.float32)  # �����Լ��е�ͼ������float32\n",
    "        loss, prediction, hash_value = test_step(model, x, y)\n",
    "        sum_loss += loss\n",
    "        sum_accuracy += accuracy(y, prediction)\n",
    "        hash_value = getBinaryTensor(hash_value)\n",
    "        hashh_train=np.append(hashh_train,hash_value)\n",
    "        labell_train=np.append(labell_train,y)\n",
    "\n",
    "        for j in range(hash_value.shape[0]):\n",
    "            filewriter.writelines(  str(hash_value[j].numpy())  + \"\\n\")\n",
    "            count += 1\n",
    "    print('test, loss:%f, accuracy:%f' %\n",
    "          (sum_loss / test_iter, sum_accuracy / test_iter))\n",
    "    return hashh_train,labell_train\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    import os\n",
    "\n",
    "    # get model\n",
    "    #img_input = Input(shape=(224, 224, 3))\n",
    "    #net = VGG19Net()\n",
    "    #net.build((None,224,224,3))\n",
    "    #output = net(img_input)\n",
    "    #model = models.Model(img_input, output)\n",
    "#     os.environ['CUDA_VISIBLE_DEVICES'] = '1'\n",
    "#     physical_devices = tf.config.experimental.list_physical_devices(device_type='GPU')\n",
    "#     tf.config.experimental.set_memory_growth(device=physical_devices[0], enable=True)\n",
    "\n",
    "    model = VGG19Net()\n",
    "    model.build((None, 224, 224, 3))\n",
    "\n",
    "    checkpoint_save_path = \"b200834cs/vgg/\"\n",
    "    checkpoint = tf.train.Checkpoint(myAwesomeModel=model)\n",
    "    print('-------------load the model-----------------')\n",
    "    if checkpoint.restore(tf.train.latest_checkpoint(checkpoint_save_path)):\n",
    "       print(\"succesful\")\n",
    "    print(\"loaded the model \" + checkpoint_save_path + \"model.ckpt-4\")\n",
    "    train_acc,traint = test(model, iterations_per_epoch,\"Downloads/train.txt\")\n",
    "    print(train_acc)\n",
    "    print(traint)\n",
    "    traint_binary_vgg=train_acc.reshape(int(len(train_acc)/512),512)\n",
    "    test_acc,testt = test(model, test_iterations,\"Downloads/test.txt\")\n",
    "    print(test_acc)\n",
    "    print(testt)\n",
    "    testt_binary_vgg=test_acc.reshape(int(len(test_acc)/512),512)\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "7bb0571f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloads/train.txt\n",
      "Downloads/test.txt\n",
      "Downloads/train.txt\n",
      "Downloads/test.txt\n",
      "Downloads/train.txt\n",
      "Downloads/test.txt\n",
      "[355, 177, 86, 177, 56, 445, 445, 263, 56, 56, 0, 355, 86, 56, 0, 177, 264, 264, 264, 0, 86, 0, 86, 355, 264, 264, 178, 56, 446, 86, 0, 178, 178, 446, 56, 0, 86, 355, 86, 178, 56, 86, 86, 446, 264, 446, 355, 86, 264, 264, 0, 178, 57, 57, 86, 0, 264, 264, 57, 178, 178, 264, 264, 57, 87, 264, 446, 178, 57, 178, 355, 87, 87, 1, 264, 57, 57, 178, 87, 57, 1, 178, 57, 57, 57, 446, 1, 87, 446, 87, 57, 57, 446, 446, 178, 355, 87, 178, 355, 264, 1, 446, 446, 87, 57, 264, 57, 57, 178, 1, 264, 265, 355, 1, 265, 355, 87, 355, 87, 1, 1, 265, 1, 1, 446, 265, 355, 87, 87, 355, 446, 178, 355, 1, 178, 178, 356, 356, 178, 356, 356, 1, 446, 57, 1, 356, 265, 57, 356, 57, 446, 356, 265, 265, 178, 265, 265, 446, 87, 356, 87, 1, 87, 87, 1, 87, 178, 265, 356, 356, 446, 356, 446, 1, 446, 58, 178, 356, 356, 178, 58, 58, 265, 1, 178, 265, 356, 178, 356, 447, 58, 1, 447, 87, 265, 447, 58, 1, 178, 447, 1, 265, 447, 356, 179, 265, 87, 87, 87, 447, 179, 1, 58, 87, 58, 1, 265, 87, 1, 265, 179, 58, 265, 58, 265, 179, 58, 447, 265, 87, 58, 58, 265, 447, 179, 356, 87, 1, 356, 265, 356, 1, 58, 58, 179, 87, 87, 356, 179, 88, 58, 447, 88, 356, 266, 447, 88, 88, 88, 179, 356, 356, 88, 88, 1, 266, 266, 88, 179, 88, 447, 58, 1, 1, 447, 58, 1, 88, 179, 88, 1, 179, 357, 1, 357, 357, 266, 179, 266, 447, 59, 447, 59, 1, 266, 357, 447, 179, 179, 88, 447, 59, 357, 179, 1, 59, 266, 447, 1, 447, 59, 179, 88, 447, 266, 179, 179, 447, 447, 266, 447, 447, 447, 266, 266, 59, 179, 179, 447, 447, 179, 357, 357, 59, 89, 266, 59, 2, 59, 357, 59, 2, 179, 266, 2, 266, 266, 59, 266, 357, 448, 266, 179, 89, 179, 357, 89, 357, 448, 2, 89, 2, 357, 89, 448, 89, 448, 89, 448, 2, 59, 2, 266, 179, 357, 179, 179, 357, 59, 266, 59, 357, 179, 179, 2, 266, 448, 59, 89, 266, 448, 357, 357, 59, 180, 267, 2, 2, 2, 2, 89, 2, 89, 448, 2, 267, 448, 267, 2, 357, 89, 448, 448, 267, 448, 448, 448, 180, 448, 180, 89, 180, 267, 267, 180, 357, 448, 448, 267, 89, 89, 3, 358, 449, 3, 89, 3, 267, 89, 3, 89, 267, 89, 358, 358, 89, 89, 267, 449, 449, 3, 89, 180, 180, 89, 89, 180, 3, 449, 180, 3, 449, 180, 89, 449, 89, 89, 358, 267, 358, 358, 89, 449, 449, 449, 180, 89, 89, 3, 3, 180, 180, 267, 358, 89, 449, 449, 449, 358, 180, 89, 180, 3, 267, 267, 358, 449, 90, 180, 449]\n",
      "187808\n",
      "9885\n"
     ]
    }
   ],
   "source": [
    "xx,traint_resnet=load_list(\"Downloads/train.txt\")\n",
    "yy,testt_resnet=load_list(\"Downloads/test.txt\")\n",
    "xx,traint_densenet=load_list(\"Downloads/train.txt\")\n",
    "yy,testt_densenet=load_list(\"Downloads/test.txt\")\n",
    "xx,traint_vgg=load_list(\"Downloads/train.txt\")\n",
    "yy,testt_vgg=load_list(\"Downloads/test.txt\")\n",
    "print(testt_resnet[:500])\n",
    "print(len(traint_resnet))\n",
    "print(len(testt_resnet))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "e34f8608",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Save array\n",
    "np.save('Downloads/vgg_train.npy', traint_binary_vgg)\n",
    "np.save('Downloads/vgg_test.npy', testt_binary_vgg)\n",
    "np.save('Downloads/resnet_train.npy', traint_binary_resnet)\n",
    "np.save('Downloads/resnet_test.npy', testt_binary_resnet)\n",
    "np.save('Downloads/densenet_train.npy', traint_binary_densenet)\n",
    "np.save('Downloads/densenet_test.npy', testt_binary_densenet)\n",
    "\n",
    "# Load array\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "1f62be1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "traintt_binary_vgg = np.load('Downloads/vgg_train.npy')\n",
    "testtt_binary_vgg = np.load('Downloads/vgg_test.npy')\n",
    "traintt_binary_resnet = np.load('Downloads/resnet_train.npy')\n",
    "testtt_binary_resnet = np.load('Downloads/resnet_test.npy')\n",
    "traintt_binary_densenet = np.load('Downloads/densenet_train.npy')\n",
    "testtt_binary_densenet = np.load('Downloads/densenet_test.npy')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "6f31fc6a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "187808\n",
      "9856\n",
      "187808\n"
     ]
    }
   ],
   "source": [
    "print(len(traintt_binary_vgg))\n",
    "print(len(testtt_binary_densenet))\n",
    "print(len(traintt_binary_resnet))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "6605ed82",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "187808\n",
      "896\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|                                                                                                 | 0/896 [00:03<?, ?it/s]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Input \u001b[0;32mIn [9]\u001b[0m, in \u001b[0;36m<cell line: 295>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    339\u001b[0m TestPath \u001b[38;5;241m=\u001b[39m [TestPath_VGG, TestPath_Resnet, TestPath_Desnet]\n\u001b[1;32m    340\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;28mlen\u001b[39m(TestLabel[\u001b[38;5;241m0\u001b[39m]))\n\u001b[0;32m--> 341\u001b[0m \u001b[38;5;28mmap\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[43mprecision\u001b[49m\u001b[43m(\u001b[49m\u001b[43m \u001b[49m\u001b[43mTestLabel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mTestData\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mTestPath\u001b[49m\u001b[43m,\u001b[49m\u001b[43mDataLabel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mDataBase\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mDataPath\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m30\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m    343\u001b[0m filewrite \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mopen\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmap_sun397_48bit_top1000754_bit.txt\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mw\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    344\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mmap\u001b[39m:\n",
      "Input \u001b[0;32mIn [9]\u001b[0m, in \u001b[0;36mprecision\u001b[0;34m(train_label, train_binary, train_base, test_label, test_binary, test_query, top_k)\u001b[0m\n\u001b[1;32m    219\u001b[0m     pic_path3[v] \u001b[38;5;241m=\u001b[39m k\n\u001b[1;32m    221\u001b[0m hanming_normalazition1 \u001b[38;5;241m=\u001b[39m hanming_normalazition(order_map_1)\n\u001b[0;32m--> 223\u001b[0m hanming_normalazition2 \u001b[38;5;241m=\u001b[39m \u001b[43mhanming_normalazition\u001b[49m\u001b[43m(\u001b[49m\u001b[43morder_map_2\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    224\u001b[0m hanming_normalazition3 \u001b[38;5;241m=\u001b[39m hanming_normalazition(order_map_3)\n\u001b[1;32m    226\u001b[0m \u001b[38;5;66;03m# print(\"hanming_normalazition1\", hanming_normalazition1)\u001b[39;00m\n\u001b[1;32m    227\u001b[0m \n\u001b[1;32m    228\u001b[0m \u001b[38;5;66;03m# 汉明距离归一化 * 精度\u001b[39;00m\n",
      "Input \u001b[0;32mIn [9]\u001b[0m, in \u001b[0;36mhanming_normalazition\u001b[0;34m(arr)\u001b[0m\n\u001b[1;32m     35\u001b[0m \u001b[38;5;66;03m# print(\"汉明归一化循环次数，\",len(arr))\u001b[39;00m\n\u001b[1;32m     36\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;28mlen\u001b[39m(arr)):\n\u001b[0;32m---> 37\u001b[0m     res\u001b[38;5;241m.\u001b[39mappend(\u001b[38;5;28;43mround\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[43m \u001b[49m\u001b[43m(\u001b[49m\u001b[43marr\u001b[49m\u001b[43m[\u001b[49m\u001b[43mi\u001b[49m\u001b[43m]\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m/\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m48\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m4\u001b[39;49m\u001b[43m)\u001b[49m)\n\u001b[1;32m     38\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m res\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "import time\n",
    "\n",
    "np.set_printoptions(threshold=np.inf) ##no of elements in an array should be displayed when we print an array\n",
    "import os\n",
    "\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0\"\n",
    "\n",
    "##calculate hamming distance between two arrays\n",
    "def hanming(v1, v2):\n",
    "    tt = 1.0 * np.sum(v1 != v2, axis=1)\n",
    "    return tt\n",
    "\n",
    "##calculating the element wise divison\n",
    "def true_divide(buffer_yes, Ns):\n",
    "    P = np.true_divide(np.cumsum(buffer_yes), np.array(Ns))\n",
    "    return P\n",
    "\n",
    "\n",
    "## return the indices of the elements in ascending order\n",
    "def argsort(similarity):\n",
    "    return np.argsort(similarity, kind='mergesort')\n",
    "\n",
    "\n",
    "def sum(arr):\n",
    "    res = 0\n",
    "    for i in range(len(arr)):\n",
    "        res += arr[i]\n",
    "    return res\n",
    "\n",
    "\n",
    "def hanming_normalazition(arr):\n",
    "    res = []\n",
    "    # print(\"汉明归一化循环次数，\",len(arr))\n",
    "    for i in range(len(arr)):\n",
    "        res.append(round(1 - (arr[i] / 48), 4))\n",
    "    return res\n",
    "    # return [round(1 - (i / 48), 4) for i in arr]\n",
    "\n",
    "\n",
    "def cumsum(arr):\n",
    "    return np.cumsum(arr)\n",
    "\n",
    "\n",
    "def zeros(K):\n",
    "    return np.zeros(K)\n",
    "\n",
    "## train_label[0], y2_1, K, test_label[0][i], buffer_yes_1\n",
    "def fun(label, y, K, query_label, buffer_yes):\n",
    "    # print(\"fun函数  循环次数：\", K)\n",
    "    for j in range(0, K):\n",
    "        # print(j)\n",
    "        # 检索出来的标签\n",
    "        retrieval_label = label[y[j]]\n",
    "        if query_label == retrieval_label:\n",
    "            buffer_yes[j] = 1\n",
    "    return buffer_yes\n",
    "\n",
    "\n",
    "def produce_hanming_list(arr):\n",
    "    temp = []\n",
    "    for i in range(len(arr)):\n",
    "        temp.append(arr[i])\n",
    "    return temp\n",
    "\n",
    "\n",
    "def hanming_matrix(hanming_normalazition):\n",
    "    hanming = []\n",
    "    for i in hanming_normalazition:\n",
    "        hanming.append(hanming_normalazition[i])\n",
    "    return hanming\n",
    "\n",
    "## contains three image paths containing k images each and it corresponding hamming distances and it will return the k images among which are least hamming distance\n",
    "def list_add(K, pic_path1, pic_path2, pic_path3, hanming_matrix1, hanming_matrix2, hanming_matrix3):\n",
    "    list1_1, list1_2, list1_3, = [], [], []\n",
    "    # 三个for循环比较慢\n",
    "    # 键 1-n   值:正确图片的索引\n",
    "    # print(\"pic_path\",pic_path1)\n",
    "    # print(pic_path1.values())\n",
    "    for k, v in pic_path1.items():\n",
    "        # print(\"pic_path1.index(p1)\",pic_path1.get(v))\n",
    "        # print(\"v\",v)\n",
    "        # print(pic_path1[k])\n",
    "        result1 = (hanming_matrix1[pic_path1[k]] + hanming_matrix2[pic_path2[k]] + hanming_matrix3[\n",
    "            pic_path3[k]]) / 3\n",
    "        list1_1.append(result1)\n",
    "    # print(list1_1)\n",
    "    for k, v in pic_path2.items():\n",
    "        result2 = (hanming_matrix1[pic_path1[k]] + hanming_matrix2[pic_path2[k]] + hanming_matrix3[\n",
    "            pic_path3[k]]) / 3\n",
    "        list1_2.append(result2)\n",
    "    for k, v in pic_path3.items():\n",
    "        result3 = (hanming_matrix1[pic_path1[k]] + hanming_matrix2[pic_path2[k]] + hanming_matrix3[\n",
    "            pic_path3[k]]) / 3\n",
    "        list1_3.append(result3)\n",
    "    path = list(pic_path1.keys())[:K] + list(pic_path2.keys())[:K] + list(pic_path3.keys())[:K]\n",
    "    new_list = list1_1 + list1_2 + list1_3\n",
    "    return path, new_list\n",
    "\n",
    "##train_label[0], train_base_dic, final_path_dic, final_index_dict\n",
    "\n",
    "def fn(train_label, train_base_dic, final_path_dic, final_index_dict):\n",
    "    # print(\"final_path\", final_path)\n",
    "    # base_x = train_base[0].tolist()\n",
    "    # base_x = [int(i) for i in base_x]   # base_x   0-49999\n",
    "    temp_x = []\n",
    "    for _, v in final_index_dict.items():\n",
    "        # index_x = final_path_dic.get(v)\n",
    "        temp_x.append(train_label[train_base_dic.get(final_path_dic.get(v))])\n",
    "\n",
    "    return temp_x[::-1]\n",
    "\n",
    "\n",
    "##DataLabel, DataBase, DataPath, TestLabel, TestData, TestPath, 1000754\n",
    "def precision(train_label, train_binary, train_base, test_label, test_binary, test_query, top_k):\n",
    "    \"\"\"\n",
    "    train_label:训练集图片的标签\n",
    "    train_binary:训练集图片的二值码\n",
    "    test_label:测试集图片的标签\n",
    "    test_binary:测试集图片的二值码\n",
    "    top_k:查询返回图片（top_k）数量的map值\n",
    "    \"\"\"\n",
    "    K = top_k\n",
    "    QueryTimes = test_binary[0].shape[0] - 128 \n",
    "    print(QueryTimes)# 获得行数 //size of the array of the first index\n",
    "    AP_1 = np.zeros((QueryTimes, 1)) # AP_1 is an array containing  zeros same as number of testing data images\n",
    "    AP_2 = np.zeros((QueryTimes, 1))\n",
    "    AP_3 = np.zeros((QueryTimes, 1))\n",
    "\n",
    "    # [1,2,....K]\n",
    "    Ns = [i for i in range(1, K + 1)]\n",
    "    train_base = [int(i) for i in train_base[0]]\n",
    "    # print(\"train_base\",train_base)\n",
    "    train_base = [int(i) for i in train_base]\n",
    "    train_base_dic = {} #dictionary with key value pairs\n",
    "    for i in range(len(train_base)):\n",
    "        train_base_dic[i] = i\n",
    "    # print(\"train_base\",train_base_dic)\n",
    "    ap1 = 0\n",
    "    for i in tqdm(range(0, QueryTimes)):\n",
    "        # for i in range(0, QueryTimes):\n",
    "        # 查询集   真实标签\n",
    "        query_label_3 = test_label[2][i]\n",
    "\n",
    "#         print(\"query_label_3\", query_label_3)\n",
    "        # print(test_query[i])\n",
    "\n",
    "        # 真实标签的哈希码\n",
    "        query_binary_1 = test_binary[0][i]  # 去一个测试二值码\n",
    "        query_binary_2 = test_binary[1][i]\n",
    "        query_binary_3 = test_binary[2][i]\n",
    "\n",
    "        # (6144,)  汉明矩阵  乱序\n",
    "\n",
    "        similarity_1 = hanming(train_binary[0], query_binary_1)\n",
    "        similarity_2 = hanming(train_binary[1], query_binary_2)\n",
    "        similarity_3 = hanming(train_binary[2], query_binary_3)\n",
    "\n",
    "        # 汉明矩阵排序，返回汉明距离从小到大的索引  即正确的索引\n",
    "\n",
    "        y2_1 = argsort(similarity_1)\n",
    "        y2_2 = argsort(similarity_2)\n",
    "        y2_3 = argsort(similarity_3)# y2 contains the index values in ascending order of the similarity values of the array\n",
    "\n",
    "        # (6144,)\n",
    "        # print(y2_1[:10])\n",
    "\n",
    "        buffer_yes_1 = zeros(K)\n",
    "        buffer_yes_2 = zeros(K)\n",
    "        buffer_yes_3 = zeros(K)\n",
    "\n",
    "        buffer_yes_1 = fun(train_label[0], y2_1, K, test_label[0][i], buffer_yes_1)  ## finds whether the test label matches with label of top k images of the trained_dataset according to hamming distance or not\n",
    "        buffer_yes_2 = fun(train_label[1], y2_2, K, test_label[1][i], buffer_yes_2) ## is an array of size k contains value 0- mismatch 1- match\n",
    "        buffer_yes_3 = fun(train_label[2], y2_3, K, test_label[2][i], buffer_yes_3)\n",
    "\n",
    "        P_1 = true_divide(buffer_yes_1, Ns)\n",
    "        P_2 = true_divide(buffer_yes_2, Ns)\n",
    "        P_3 = true_divide(buffer_yes_3, Ns)\n",
    "\n",
    "        if sum(buffer_yes_1) == 0:\n",
    "            AP_1[i] = 0\n",
    "        else:\n",
    "            AP_1[i] = sum(np.array(P_1) * np.array(buffer_yes_1)) / sum(buffer_yes_1)\n",
    "\n",
    "        if sum(buffer_yes_2) == 0:\n",
    "            AP_2[i] = 0\n",
    "        else:\n",
    "            AP_2[i] = sum(np.array(P_2) * np.array(buffer_yes_2)) / sum(buffer_yes_2)\n",
    "\n",
    "        if sum(buffer_yes_3) == 0:\n",
    "            AP_3[i] = 0\n",
    "        else:\n",
    "            AP_3[i] = sum(np.array(P_3) * np.array(buffer_yes_3)) / sum(buffer_yes_3)\n",
    "\n",
    "        # 排序后的汉明距离矩阵\n",
    "\n",
    "        order_map_1 = [similarity_1[i] for i in y2_1] ## contain values of hamming distance in ascending order\n",
    "        # print(\"order_map_1\", order_map_1)\n",
    "        order_map_2 = [similarity_2[i] for i in y2_2]\n",
    "        order_map_3 = [similarity_3[i] for i in y2_3]\n",
    "\n",
    "        pic_path1 = {}\n",
    "        pic_path2 = {}\n",
    "        pic_path3 = {}\n",
    "\n",
    "        for k, v in enumerate(y2_1, 0):\n",
    "            # print(\"k\",k)\n",
    "            # print(\"v\",v)\n",
    "            pic_path1[v] = k  ## value- position of the similarity value  key-- index value\n",
    "\n",
    "        # for step,k in enumerate(y2_1):\n",
    "        #     print(\"step\",step)\n",
    "        #     print(\"k\",k)\n",
    "        #     pic_path1[k] = step\n",
    "        for k, v in enumerate(y2_2, 0):\n",
    "            pic_path2[v] = k\n",
    "        for k, v in enumerate(y2_3, 0):\n",
    "            pic_path3[v] = k\n",
    "\n",
    "        hanming_normalazition1 = hanming_normalazition(order_map_1)\n",
    "\n",
    "        hanming_normalazition2 = hanming_normalazition(order_map_2)\n",
    "        hanming_normalazition3 = hanming_normalazition(order_map_3)\n",
    "\n",
    "        # print(\"hanming_normalazition1\", hanming_normalazition1)\n",
    "\n",
    "        # 汉明距离归一化 * 精度\n",
    "        hanming_matrix1 = [i * acc[0] for i in hanming_normalazition1]\n",
    "        hanming_matrix2 = [i * acc[1] for i in hanming_normalazition2]\n",
    "        hanming_matrix3 = [i * acc[2] for i in hanming_normalazition3]\n",
    "\n",
    "        path, new_list = list_add(K, pic_path1, pic_path2, pic_path3, hanming_matrix1, hanming_matrix2, hanming_matrix3)\n",
    "\n",
    "        t1 = time.time()\n",
    "\n",
    "        dictionary = {}\n",
    "        for step, p in enumerate(path):\n",
    "            value = dictionary.get(p, [])\n",
    "            value.append(step)\n",
    "            dictionary[p] = value\n",
    "        # print(\"dictionary\",dictionary)\n",
    "        path_arr = [None for _ in range(len(path))]\n",
    "        num_arr = [0 for _ in range(len(path))]\n",
    "\n",
    "        for k, v in dictionary.items():\n",
    "            path_arr[v[0]] = k\n",
    "            num_arr[v[0]] = new_list[v[0]]\n",
    "\n",
    "        final_path = [i for i in path_arr if i is not None][:K]\n",
    "        final_path_dic = {}\n",
    "        for step, r in enumerate(final_path):\n",
    "            final_path_dic[step] = r\n",
    "        # print(\"final_path_dic\",final_path_dic)\n",
    "        final_num = [i for i in num_arr if i][:K]\n",
    "\n",
    "        final_index = argsort(final_num)\n",
    "        final_index_dict = {}\n",
    "        for step, r in enumerate(final_index):\n",
    "            final_index_dict[step] = r\n",
    "\n",
    "        lable_final = fn(train_label[0], train_base_dic, final_path_dic, final_index_dict)\n",
    "\n",
    "#         print(\"lable_final\", lable_final)\n",
    "        count = 0.0\n",
    "        sum_value = 0.0\n",
    "        for step, r in enumerate(lable_final[:top_k]):\n",
    "            step += 1\n",
    "#             print(r)\n",
    "            if str(query_label_3) == str(r):\n",
    "                count += 1\n",
    "                sum_value += count / step\n",
    "        if count :\n",
    "#             print('@@@@@')\n",
    "            ap = sum_value / count\n",
    "        else:\n",
    "            ap = 0\n",
    "        ap1 += ap\n",
    "#         print(\"ap\", ap)\n",
    "#         print(\"AP_1[i]\", AP_1[i])\n",
    "#         print(\"AP_2[i]\", AP_2[i])\n",
    "#         print(\"AP_3[i]\", AP_3[i])\n",
    "        t2 = time.time()\n",
    "        #print(\"AP时间\", t2 - t1)\n",
    "\n",
    "    map_1 = np.mean(AP_1)\n",
    "    map_2 = np.mean(AP_2)\n",
    "    map_3 = np.mean(AP_3)\n",
    "    ap1 = ap1 / QueryTimes\n",
    "\n",
    "    # return precision_at_k, map\n",
    "    return map_1, map_2, map_3, ap1\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    # vgg resnet dense\n",
    "    acc = [0.669, 0.692, 0.705]\n",
    "    # 测试集\n",
    "    # F:\\project\\produce_hash\\NPY\n",
    "    # F:\\project\\produce_hash\\MAP_集成.py\n",
    "#     traint_binary_densenet=traint_binary_resnet\n",
    "#     traint_densenet=traint_resnet\n",
    "#     testt_binary_densenet=testt_binary_resnet\n",
    "#     testt_densenet=testt_resnet\n",
    "    \n",
    "    \n",
    "    TestData_VGG = traintt_binary_vgg\n",
    "    TestLabel_VGG = traint_vgg\n",
    "    TestPath_VGG = traint_vgg\n",
    "    # 训练集\n",
    "    DataBase_VGG = testtt_binary_vgg[:1024]\n",
    "    DataLabel_VGG = testt_vgg[:1024]\n",
    "    DataPath_VGG = testt_vgg[:1024]\n",
    "\n",
    "    # 测试集\n",
    "    TestData_Resnet = traintt_binary_resnet\n",
    "    TestLabel_Resnet = traint_resnet\n",
    "    TestPath_Resnet = traint_resnet\n",
    "    # 训练集\n",
    "    DataBase_Resnet = testtt_binary_resnet[:1024]\n",
    "    DataLabel_Resnet = testt_resnet[:1024]\n",
    "    DataPath_Resnet = testt_resnet[:1024]\n",
    "\n",
    "    # 测试集\n",
    "    TestData_Desnet = traintt_binary_densenet\n",
    "    TestLabel_Desnet = traint_densenet\n",
    "    TestPath_Desnet = traint_densenet\n",
    "    # 训练集\n",
    "    DataBase_Desnet = testtt_binary_densenet[:1024]\n",
    "    DataLabel_Desnet = testt_densenet[:1024]\n",
    "    DataPath_Desnet = testt_densenet[:1024]\n",
    "\n",
    "    DataLabel = [DataLabel_VGG, DataLabel_Resnet, DataLabel_Desnet]\n",
    "    DataBase = [DataBase_VGG, DataBase_Resnet, DataBase_Desnet]\n",
    "    DataPath = [DataPath_VGG, DataPath_Resnet, DataPath_Desnet]\n",
    "\n",
    "    TestLabel = [TestLabel_VGG, TestLabel_Resnet, TestLabel_Desnet]\n",
    "    TestData = [TestData_VGG, TestData_Resnet, TestData_Desnet]\n",
    "    TestPath = [TestPath_VGG, TestPath_Resnet, TestPath_Desnet]\n",
    "    print(len(TestLabel[0]))\n",
    "    map = precision( TestLabel, TestData, TestPath,DataLabel, DataBase, DataPath, 30)\n",
    "\n",
    "    filewrite = open(\"map_sun397_48bit_top1000754_bit.txt\", \"w\")\n",
    "    for i in map:\n",
    "        filewrite.write(\"map: \" + str(i) + \"\\n\")\n",
    "        filewrite.write(\"bit_num: \" + \"48\\n\")\n",
    "        print(i)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bcdec30f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "187808\n",
      "896\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 90%|████████████████████████████████████████████████████████████████████████████▏        | 803/896 [1:32:33<10:09,  6.56s/it]"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "import time\n",
    "\n",
    "np.set_printoptions(threshold=np.inf) ##no of elements in an array should be displayed when we print an array\n",
    "import os\n",
    "\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0\"\n",
    "\n",
    "##calculate hamming distance between two arrays\n",
    "def hanming(v1, v2):\n",
    "    tt = 1.0 * np.sum(v1 != v2, axis=1)\n",
    "    return tt\n",
    "\n",
    "##calculating the element wise divison\n",
    "def true_divide(buffer_yes, Ns):\n",
    "    P = np.true_divide(np.cumsum(buffer_yes), np.array(Ns))\n",
    "    return P\n",
    "\n",
    "\n",
    "## return the indices of the elements in ascending order\n",
    "def argsort(similarity):\n",
    "    return np.argsort(similarity, kind='mergesort')\n",
    "\n",
    "\n",
    "def sum(arr):\n",
    "    res = 0\n",
    "    for i in range(len(arr)):\n",
    "        res += arr[i]\n",
    "    return res\n",
    "\n",
    "\n",
    "def hanming_normalazition(arr):\n",
    "    res = []\n",
    "    # print(\"汉明归一化循环次数，\",len(arr))\n",
    "    for i in range(len(arr)):\n",
    "        res.append(round(1 - (arr[i] / 48), 4))\n",
    "    return res\n",
    "    # return [round(1 - (i / 48), 4) for i in arr]\n",
    "\n",
    "\n",
    "def cumsum(arr):\n",
    "    return np.cumsum(arr)\n",
    "\n",
    "\n",
    "def zeros(K):\n",
    "    return np.zeros(K)\n",
    "\n",
    "## train_label[0], y2_1, K, test_label[0][i], buffer_yes_1\n",
    "def fun(label, y, K, query_label, buffer_yes):\n",
    "    # print(\"fun函数  循环次数：\", K)\n",
    "    for j in range(0, K):\n",
    "        # print(j)\n",
    "        # 检索出来的标签\n",
    "        retrieval_label = label[y[j]]\n",
    "        if query_label == retrieval_label:\n",
    "            buffer_yes[j] = 1\n",
    "    return buffer_yes\n",
    "\n",
    "\n",
    "def produce_hanming_list(arr):\n",
    "    temp = []\n",
    "    for i in range(len(arr)):\n",
    "        temp.append(arr[i])\n",
    "    return temp\n",
    "\n",
    "\n",
    "def hanming_matrix(hanming_normalazition):\n",
    "    hanming = []\n",
    "    for i in hanming_normalazition:\n",
    "        hanming.append(hanming_normalazition[i])\n",
    "    return hanming\n",
    "\n",
    "## contains three image paths containing k images each and it corresponding hamming distances and it will return the k images among which are least hamming distance\n",
    "def list_add(K, pic_path1, pic_path2, pic_path3, hanming_matrix1, hanming_matrix2, hanming_matrix3):\n",
    "    list1_1, list1_2, list1_3, = [], [], []\n",
    "    # 三个for循环比较慢\n",
    "    # 键 1-n   值:正确图片的索引\n",
    "    # print(\"pic_path\",pic_path1)\n",
    "    # print(pic_path1.values())\n",
    "    for k, v in pic_path1.items():\n",
    "        # print(\"pic_path1.index(p1)\",pic_path1.get(v))\n",
    "        # print(\"v\",v)\n",
    "        # print(pic_path1[k])\n",
    "        result1 = (hanming_matrix1[pic_path1[k]] + hanming_matrix2[pic_path2[k]] + hanming_matrix3[\n",
    "            pic_path3[k]]) / 3\n",
    "        list1_1.append(result1)\n",
    "    # print(list1_1)\n",
    "    for k, v in pic_path2.items():\n",
    "        result2 = (hanming_matrix1[pic_path1[k]] + hanming_matrix2[pic_path2[k]] + hanming_matrix3[\n",
    "            pic_path3[k]]) / 3\n",
    "        list1_2.append(result2)\n",
    "    for k, v in pic_path3.items():\n",
    "        result3 = (hanming_matrix1[pic_path1[k]] + hanming_matrix2[pic_path2[k]] + hanming_matrix3[\n",
    "            pic_path3[k]]) / 3\n",
    "        list1_3.append(result3)\n",
    "    path = list(pic_path1.keys())[:K] + list(pic_path2.keys())[:K] + list(pic_path3.keys())[:K]\n",
    "    new_list = list1_1 + list1_2 + list1_3\n",
    "    return path, new_list\n",
    "\n",
    "##train_label[0], train_base_dic, final_path_dic, final_index_dict\n",
    "\n",
    "def fn(train_label, train_base_dic, final_path_dic, final_index_dict):\n",
    "    # print(\"final_path\", final_path)\n",
    "    # base_x = train_base[0].tolist()\n",
    "    # base_x = [int(i) for i in base_x]   # base_x   0-49999\n",
    "    temp_x = []\n",
    "    for _, v in final_index_dict.items():\n",
    "        # index_x = final_path_dic.get(v)\n",
    "        temp_x.append(train_label[train_base_dic.get(final_path_dic.get(v))])\n",
    "\n",
    "    return temp_x[::-1]\n",
    "\n",
    "\n",
    "##DataLabel, DataBase, DataPath, TestLabel, TestData, TestPath, 1000754\n",
    "def precision(train_label, train_binary, train_base, test_label, test_binary, test_query, top_k):\n",
    "    \"\"\"\n",
    "    train_label:训练集图片的标签\n",
    "    train_binary:训练集图片的二值码\n",
    "    test_label:测试集图片的标签\n",
    "    test_binary:测试集图片的二值码\n",
    "    top_k:查询返回图片（top_k）数量的map值\n",
    "    \"\"\"\n",
    "    K = top_k\n",
    "    QueryTimes = test_binary[0].shape[0] - 128 \n",
    "    print(QueryTimes)# 获得行数 //size of the array of the first index\n",
    "    AP_1 = np.zeros((QueryTimes, 1)) # AP_1 is an array containing  zeros same as number of testing data images\n",
    "    AP_2 = np.zeros((QueryTimes, 1))\n",
    "    AP_3 = np.zeros((QueryTimes, 1))\n",
    "    recal_1= np.zeros((QueryTimes, 1))\n",
    "    recal_2= np.zeros((QueryTimes, 1)) \n",
    "    recal_3= np.zeros((QueryTimes, 1))\n",
    "    f1_1= np.zeros((QueryTimes, 1))\n",
    "    f1_2= np.zeros((QueryTimes, 1))\n",
    "    f1_3= np.zeros((QueryTimes, 1))\n",
    "    # [1,2,....K]\n",
    "    Ns = [i for i in range(1, K + 1)]\n",
    "    train_base = [int(i) for i in train_base[0]]\n",
    "    # print(\"train_base\",train_base)\n",
    "    train_base = [int(i) for i in train_base]\n",
    "    train_base_dic = {} #dictionary with key value pairs\n",
    "    for i in range(len(train_base)):\n",
    "        train_base_dic[i] = i\n",
    "    # print(\"train_base\",train_base_dic)\n",
    "    ap1 = 0\n",
    "    recal = 0\n",
    "    F1score = 0\n",
    "#     F1score\n",
    "    for i in tqdm(range(0, QueryTimes)):\n",
    "        # for i in range(0, QueryTimes):\n",
    "        # 查询集   真实标签\n",
    "        query_label_3 = test_label[2][i]\n",
    "\n",
    "#         print(\"query_label_3\", query_label_3)\n",
    "        # print(test_query[i])\n",
    "\n",
    "        # 真实标签的哈希码\n",
    "        query_binary_1 = test_binary[0][i]  # 去一个测试二值码\n",
    "        query_binary_2 = test_binary[1][i]\n",
    "        query_binary_3 = test_binary[2][i]\n",
    "\n",
    "        # (6144,)  汉明矩阵  乱序\n",
    "\n",
    "        similarity_1 = hanming(train_binary[0], query_binary_1)\n",
    "        similarity_2 = hanming(train_binary[1], query_binary_2)\n",
    "        similarity_3 = hanming(train_binary[2], query_binary_3)\n",
    "\n",
    "        # 汉明矩阵排序，返回汉明距离从小到大的索引  即正确的索引\n",
    "\n",
    "        y2_1 = argsort(similarity_1)\n",
    "        y2_2 = argsort(similarity_2)\n",
    "        y2_3 = argsort(similarity_3)# y2 contains the index values in ascending order of the similarity values of the array\n",
    "\n",
    "        # (6144,)\n",
    "        # print(y2_1[:10])\n",
    "\n",
    "        buffer_yes_1 = zeros(K)\n",
    "        buffer_yes_2 = zeros(K)\n",
    "        buffer_yes_3 = zeros(K)\n",
    "\n",
    "        buffer_yes_1 = fun(train_label[0], y2_1, K, test_label[0][i], buffer_yes_1)  ## finds whether the test label matches with label of top k images of the trained_dataset according to hamming distance or not\n",
    "        buffer_yes_2 = fun(train_label[1], y2_2, K, test_label[1][i], buffer_yes_2) ## is an array of size k contains value 0- mismatch 1- match\n",
    "        buffer_yes_3 = fun(train_label[2], y2_3, K, test_label[2][i], buffer_yes_3)\n",
    "\n",
    "        P_1 = true_divide(buffer_yes_1, Ns)\n",
    "        P_2 = true_divide(buffer_yes_2, Ns)\n",
    "        P_3 = true_divide(buffer_yes_3, Ns)\n",
    "\n",
    "        if sum(buffer_yes_1) == 0:\n",
    "            AP_1[i] = 0\n",
    "            recal_1[i]=0\n",
    "            f1_1[i]=0\n",
    "        else:\n",
    "            AP_1[i] = sum(np.array(P_1) * np.array(buffer_yes_1)) / sum(buffer_yes_1)\n",
    "            recal_1[i]=AP_1[i]*sum(buffer_yes_1)/400\n",
    "            f1_1[i]=2*AP_1[i]*recal_1[i]/(AP_1[i]+recal_1[i])\n",
    "\n",
    "        if sum(buffer_yes_2) == 0:\n",
    "            AP_2[i] = 0\n",
    "            recal_2[i]=0\n",
    "            f1_2[i]=0\n",
    "        else:\n",
    "            AP_2[i] = sum(np.array(P_2) * np.array(buffer_yes_2)) / sum(buffer_yes_2)\n",
    "            recal_2[i]=AP_2[i]*sum(buffer_yes_2)/400\n",
    "            f1_2[i]=2*AP_2[i]*recal_2[i]/(AP_2[i]+recal_2[i])\n",
    "        if sum(buffer_yes_3) == 0:\n",
    "            AP_3[i] = 0\n",
    "            recal_2[i]=0\n",
    "            f1_2[i]=0\n",
    "        else:\n",
    "            AP_3[i] = sum(np.array(P_3) * np.array(buffer_yes_3)) / sum(buffer_yes_3)\n",
    "            recal_3[i]=AP_3[i]*sum(buffer_yes_3)/400\n",
    "            f1_3[i]=2*AP_3[i]*recal_3[i]/(AP_3[i]+recal_3[i])\n",
    "\n",
    "        # 排序后的汉明距离矩阵\n",
    "\n",
    "        order_map_1 = [similarity_1[i] for i in y2_1] ## contain values of hamming distance in ascending order\n",
    "        # print(\"order_map_1\", order_map_1)\n",
    "        order_map_2 = [similarity_2[i] for i in y2_2]\n",
    "        order_map_3 = [similarity_3[i] for i in y2_3]\n",
    "\n",
    "        pic_path1 = {}\n",
    "        pic_path2 = {}\n",
    "        pic_path3 = {}\n",
    "\n",
    "        for k, v in enumerate(y2_1, 0):\n",
    "            # print(\"k\",k)\n",
    "            # print(\"v\",v)\n",
    "            pic_path1[v] = k  ## value- position of the similarity value  key-- index value\n",
    "\n",
    "        # for step,k in enumerate(y2_1):\n",
    "        #     print(\"step\",step)\n",
    "        #     print(\"k\",k)\n",
    "        #     pic_path1[k] = step\n",
    "        for k, v in enumerate(y2_2, 0):\n",
    "            pic_path2[v] = k\n",
    "        for k, v in enumerate(y2_3, 0):\n",
    "            pic_path3[v] = k\n",
    "\n",
    "        hanming_normalazition1 = hanming_normalazition(order_map_1)\n",
    "\n",
    "        hanming_normalazition2 = hanming_normalazition(order_map_2)\n",
    "        hanming_normalazition3 = hanming_normalazition(order_map_3)\n",
    "\n",
    "        # print(\"hanming_normalazition1\", hanming_normalazition1)\n",
    "\n",
    "        # 汉明距离归一化 * 精度\n",
    "        hanming_matrix1 = [i * acc[0] for i in hanming_normalazition1]\n",
    "        hanming_matrix2 = [i * acc[1] for i in hanming_normalazition2]\n",
    "        hanming_matrix3 = [i * acc[2] for i in hanming_normalazition3]\n",
    "\n",
    "        path, new_list = list_add(K, pic_path1, pic_path2, pic_path3, hanming_matrix1, hanming_matrix2, hanming_matrix3)\n",
    "\n",
    "        t1 = time.time()\n",
    "\n",
    "        dictionary = {}\n",
    "        for step, p in enumerate(path):\n",
    "            value = dictionary.get(p, [])\n",
    "            value.append(step)\n",
    "            dictionary[p] = value\n",
    "        # print(\"dictionary\",dictionary)\n",
    "        path_arr = [None for _ in range(len(path))]\n",
    "        num_arr = [0 for _ in range(len(path))]\n",
    "\n",
    "        for k, v in dictionary.items():\n",
    "            path_arr[v[0]] = k\n",
    "            num_arr[v[0]] = new_list[v[0]]\n",
    "\n",
    "        final_path = [i for i in path_arr if i is not None][:K]\n",
    "        final_path_dic = {}\n",
    "        for step, r in enumerate(final_path):\n",
    "            final_path_dic[step] = r\n",
    "        # print(\"final_path_dic\",final_path_dic)\n",
    "        final_num = [i for i in num_arr if i][:K]\n",
    "\n",
    "        final_index = argsort(final_num)\n",
    "        final_index_dict = {}\n",
    "        for step, r in enumerate(final_index):\n",
    "            final_index_dict[step] = r\n",
    "\n",
    "        lable_final = fn(train_label[0], train_base_dic, final_path_dic, final_index_dict)\n",
    "\n",
    "#         print(\"lable_final\", lable_final)\n",
    "        count = 0.0\n",
    "        sum_value = 0.0\n",
    "        for step, r in enumerate(lable_final[:top_k]):\n",
    "            step += 1\n",
    "#             print(r)\n",
    "            if str(query_label_3) == str(r):\n",
    "                count += 1\n",
    "                sum_value += count / step\n",
    "        if count :\n",
    "#             print('@@@@@')\n",
    "            ap = sum_value / count\n",
    "        else:\n",
    "            ap = 0\n",
    "        ap1 += ap\n",
    "        recal+= ap*count /(400)\n",
    "        F1score+= 2*ap*recal/(ap+recal)\n",
    "#         print(\"ap\", ap)\n",
    "#         print(\"AP_1[i]\", AP_1[i])\n",
    "#         print(\"AP_2[i]\", AP_2[i])\n",
    "#         print(\"AP_3[i]\", AP_3[i])\n",
    "        t2 = time.time()\n",
    "        #print(\"AP时间\", t2 - t1)\n",
    "\n",
    "    map_1 = np.mean(AP_1)\n",
    "    map_2 = np.mean(AP_2)\n",
    "    map_3 = np.mean(AP_3)\n",
    "    print(\"recall for VGG\")\n",
    "    print(np.mean(recal_1))\n",
    "    print(\"recall for Resnet\")\n",
    "    \n",
    "    print(np.mean(recal_2))\n",
    "    print(\"recall for Densenet\")\n",
    "    \n",
    "    print(np.mean(recal_3))\n",
    "#     print(\"F1score for VGG\")\n",
    "    \n",
    "#     print(np.mean(f1_1))\n",
    "#     print(\"F1score for Resnet\")\n",
    "    \n",
    "#     print(np.mean(f1_2))\n",
    "#     print(\"F1score for Densenet\")\n",
    "    \n",
    "#     print(np.mean(f1_3))\n",
    "    ap1 = ap1 / QueryTimes\n",
    "    recal=recal/QueryTimes\n",
    "    F1score=F1score/QueryTimes\n",
    "    print(recal,\"recall for ensemble model\")\n",
    "#     print(F1score,\"F1score for ensemble model\")\n",
    "    # return precision_at_k, map\n",
    "    return map_1, map_2, map_3, ap1\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    # vgg resnet dense\n",
    "    acc = [0.669, 0.692, 0.705]\n",
    "    # 测试集\n",
    "    # F:\\project\\produce_hash\\NPY\n",
    "    # F:\\project\\produce_hash\\MAP_集成.py\n",
    "#     traint_binary_densenet=traint_binary_resnet\n",
    "#     traint_densenet=traint_resnet\n",
    "#     testt_binary_densenet=testt_binary_resnet\n",
    "#     testt_densenet=testt_resnet\n",
    "    \n",
    "    \n",
    "    TestData_VGG = traintt_binary_vgg\n",
    "    TestLabel_VGG = traint_vgg\n",
    "    TestPath_VGG = traint_vgg\n",
    "    # 训练集\n",
    "    DataBase_VGG = testtt_binary_vgg[:1024]\n",
    "    DataLabel_VGG = testt_vgg[:1024]\n",
    "    DataPath_VGG = testt_vgg[:1024]\n",
    "\n",
    "    # 测试集\n",
    "    TestData_Resnet = traintt_binary_resnet\n",
    "    TestLabel_Resnet = traint_resnet\n",
    "    TestPath_Resnet = traint_resnet\n",
    "    # 训练集\n",
    "    DataBase_Resnet = testtt_binary_resnet[:1024]\n",
    "    DataLabel_Resnet = testt_resnet[:1024]\n",
    "    DataPath_Resnet = testt_resnet[:1024]\n",
    "\n",
    "    # 测试集\n",
    "    TestData_Desnet = traintt_binary_densenet\n",
    "    TestLabel_Desnet = traint_densenet\n",
    "    TestPath_Desnet = traint_densenet\n",
    "    # 训练集\n",
    "    DataBase_Desnet = testtt_binary_densenet[:1024]\n",
    "    DataLabel_Desnet = testt_densenet[:1024]\n",
    "    DataPath_Desnet = testt_densenet[:1024]\n",
    "\n",
    "    DataLabel = [DataLabel_VGG, DataLabel_Resnet, DataLabel_Desnet]\n",
    "    DataBase = [DataBase_VGG, DataBase_Resnet, DataBase_Desnet]\n",
    "    DataPath = [DataPath_VGG, DataPath_Resnet, DataPath_Desnet]\n",
    "\n",
    "    TestLabel = [TestLabel_VGG, TestLabel_Resnet, TestLabel_Desnet]\n",
    "    TestData = [TestData_VGG, TestData_Resnet, TestData_Desnet]\n",
    "    TestPath = [TestPath_VGG, TestPath_Resnet, TestPath_Desnet]\n",
    "    print(len(TestLabel[0]))\n",
    "    map = precision( TestLabel, TestData, TestPath,DataLabel, DataBase, DataPath, 200)\n",
    "\n",
    "    filewrite = open(\"map_sun397_48bit_top1000754_bit.txt\", \"w\")\n",
    "    for i in map:\n",
    "        filewrite.write(\"map: \" + str(i) + \"\\n\")\n",
    "        filewrite.write(\"bit_num: \" + \"48\\n\")\n",
    "        print(i)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e8f4d11",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
