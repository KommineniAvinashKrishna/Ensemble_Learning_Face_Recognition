{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9c440bbc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Deprecation warnings have been disabled. Set TF_ENABLE_DEPRECATION_WARNINGS=1 to re-enable them.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-05-02 19:29:41.188092: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcudart.so.11.0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /tmp/ipykernel_562/3908576244.py:2: The name tf.enable_eager_execution is deprecated. Please use tf.compat.v1.enable_eager_execution instead.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "tf.enable_eager_execution()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "debf4b51",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files train.txt and test.txt generated successfully.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import random\n",
    "\n",
    "# Function to load images and corresponding labels\n",
    "def load_images_and_labels(folder_path):\n",
    "    images = []\n",
    "    labels = []\n",
    "    label_names = []\n",
    "\n",
    "    # Iterate over folders (labels)\n",
    "    for label_folder in os.listdir(folder_path):\n",
    "        label_path = os.path.join(folder_path, label_folder)\n",
    "\n",
    "        # Skip if it's not a directory\n",
    "        if not os.path.isdir(label_path):\n",
    "            continue\n",
    "\n",
    "        # Append label name to the list\n",
    "        label_names.append(label_folder)\n",
    "\n",
    "        # Load images from the label folder\n",
    "        for image_file in os.listdir(label_path):\n",
    "            image_path = os.path.join(label_path, image_file)\n",
    "            # Append image path to the list\n",
    "            images.append(image_path)\n",
    "            # Append label index to the labels list\n",
    "            labels.append(len(label_names) - 1)  # Index of label name\n",
    "\n",
    "    return images, labels, label_names\n",
    "\n",
    "# Load images and labels\n",
    "vggface2_folder = \"Downloads/val\"\n",
    "vggface2_folder1=\"Downloads/test\"\n",
    "images, labels, label_names = load_images_and_labels(vggface2_folder)\n",
    "images1,labels1,label_names1=load_images_and_labels(vggface2_folder1)\n",
    "images=images+images1\n",
    "labels=labels+labels1\n",
    "label_names=label_names1\n",
    "\n",
    "\n",
    "# Split data into train and test sets (adjust the split ratio as needed)\n",
    "split_ratio = 0.8\n",
    "num_images = len(images)\n",
    "num_train = int(num_images * split_ratio)\n",
    "random.seed(42)\n",
    "train_indices = random.sample(range(num_images), num_train)\n",
    "test_indices = list(set(range(num_images)) - set(train_indices))\n",
    "\n",
    "# Write image paths and labels into train.txt and test.txt files\n",
    "def write_to_file(file_path, image_paths, image_labels):\n",
    "    with open(file_path, 'w') as file:\n",
    "        for path, label in zip(image_paths, image_labels):\n",
    "            file.write(f\"{path} {label}\\n\")\n",
    "\n",
    "write_to_file(\"Downloads/vgg_train.txt\", [images[i] for i in train_indices], [labels[i] for i in train_indices])\n",
    "write_to_file(\"Downloads/vgg_test.txt\", [images[i] for i in test_indices], [labels[i] for i in test_indices])\n",
    "\n",
    "print(\"Files train.txt and test.txt generated successfully.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "fd5e3942",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "158154\n",
      "1236\n",
      "39539\n"
     ]
    }
   ],
   "source": [
    "\n",
    "weight_decay = 1e-4\n",
    "epoch_num = 50\n",
    "hash_bit = 512\n",
    "num_class = len(set(labels))\n",
    "input_shape = (224, 224, 3)\n",
    "label_smoothing = 0.1\n",
    "\n",
    "# training config\n",
    "batch_size = 128\n",
    "train_num = len(train_indices) #6149\n",
    "print(train_num)\n",
    "iterations_per_epoch = int(train_num / batch_size) + 1\n",
    "warm_iterations = iterations_per_epoch\n",
    "print(iterations_per_epoch)\n",
    "\n",
    "\n",
    "initial_learning_rate = 0.05\n",
    "minimum_learning_rate = 0.0001\n",
    "\n",
    "# test config\n",
    "test_batch_size = 128\n",
    "test_num = len(test_indices)\n",
    "print(test_num)\n",
    "test_iterations = int(test_num / test_batch_size) + 1\n",
    "\n",
    "log_file = 'result/vggface2/vgg.txt'\n",
    "\n",
    "# lr\n",
    "learning_rate = [0.1, 0.01, 0.001]\n",
    "boundaries = [80 * iterations_per_epoch, 120 * iterations_per_epoch]\n",
    "\n",
    "\n",
    "# path\n",
    "train_data_path = ''\n",
    "test_data_path = ''\n",
    "imagenet_train_path = 'Downloads/vgg_train.txt'\n",
    "imagenet_test_path = 'Downloads/vgg_test.txt'\n",
    "\n",
    "\n",
    "short_side_scale = (256, 384)\n",
    "aspect_ratio_scale = (0.8, 1.25)\n",
    "hue_delta = (-36, 36)\n",
    "saturation_scale = (0.6, 1.4)\n",
    "brightness_scale = (0.6, 1.4)\n",
    "pca_std = 0.1\n",
    "\n",
    "mean = [103.939, 116.779, 123.68]\n",
    "std = [58.393, 57.12, 57.375]\n",
    "eigval = [55.46, 4.794, 1.148]\n",
    "eigvec = [[-0.5836, -0.6948, 0.4203],\n",
    "          [-0.5808, -0.0045, -0.8140],\n",
    "          [-0.5675, 0.7192, 0.4009]]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "bbc0e1ee",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting opencv-python\n",
      "  Downloading opencv_python-4.9.0.80-cp37-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (62.2 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m62.2/62.2 MB\u001b[0m \u001b[31m6.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0mm\n",
      "\u001b[?25hRequirement already satisfied: numpy>=1.17.0 in /usr/local/lib/python3.8/dist-packages (from opencv-python) (1.21.1)\n",
      "Installing collected packages: opencv-python\n",
      "Successfully installed opencv-python-4.9.0.80\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0m--- Logging error ---\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/local/lib/python3.8/dist-packages/pip/_internal/utils/logging.py\", line 177, in emit\n",
      "    self.console.print(renderable, overflow=\"ignore\", crop=False, style=style)\n",
      "  File \"/usr/local/lib/python3.8/dist-packages/pip/_vendor/rich/console.py\", line 1673, in print\n",
      "    extend(render(renderable, render_options))\n",
      "  File \"/usr/local/lib/python3.8/dist-packages/pip/_vendor/rich/console.py\", line 1305, in render\n",
      "    for render_output in iter_render:\n",
      "  File \"/usr/local/lib/python3.8/dist-packages/pip/_internal/utils/logging.py\", line 134, in __rich_console__\n",
      "    for line in lines:\n",
      "  File \"/usr/local/lib/python3.8/dist-packages/pip/_vendor/rich/segment.py\", line 249, in split_lines\n",
      "    for segment in segments:\n",
      "  File \"/usr/local/lib/python3.8/dist-packages/pip/_vendor/rich/console.py\", line 1283, in render\n",
      "    renderable = rich_cast(renderable)\n",
      "  File \"/usr/local/lib/python3.8/dist-packages/pip/_vendor/rich/protocol.py\", line 36, in rich_cast\n",
      "    renderable = cast_method()\n",
      "  File \"/usr/local/lib/python3.8/dist-packages/pip/_internal/self_outdated_check.py\", line 130, in __rich__\n",
      "    pip_cmd = get_best_invocation_for_this_pip()\n",
      "  File \"/usr/local/lib/python3.8/dist-packages/pip/_internal/utils/entrypoints.py\", line 58, in get_best_invocation_for_this_pip\n",
      "    if found_executable and os.path.samefile(\n",
      "  File \"/usr/lib/python3.8/genericpath.py\", line 101, in samefile\n",
      "    s2 = os.stat(f2)\n",
      "FileNotFoundError: [Errno 2] No such file or directory: '/usr/bin/pip'\n",
      "Call stack:\n",
      "  File \"/usr/local/bin/pip\", line 8, in <module>\n",
      "    sys.exit(main())\n",
      "  File \"/usr/local/lib/python3.8/dist-packages/pip/_internal/cli/main.py\", line 70, in main\n",
      "    return command.main(cmd_args)\n",
      "  File \"/usr/local/lib/python3.8/dist-packages/pip/_internal/cli/base_command.py\", line 101, in main\n",
      "    return self._main(args)\n",
      "  File \"/usr/local/lib/python3.8/dist-packages/pip/_internal/cli/base_command.py\", line 223, in _main\n",
      "    self.handle_pip_version_check(options)\n",
      "  File \"/usr/local/lib/python3.8/dist-packages/pip/_internal/cli/req_command.py\", line 190, in handle_pip_version_check\n",
      "    pip_self_version_check(session, options)\n",
      "  File \"/usr/local/lib/python3.8/dist-packages/pip/_internal/self_outdated_check.py\", line 236, in pip_self_version_check\n",
      "    logger.warning(\"[present-rich] %s\", upgrade_prompt)\n",
      "  File \"/usr/lib/python3.8/logging/__init__.py\", line 1458, in warning\n",
      "    self._log(WARNING, msg, args, **kwargs)\n",
      "  File \"/usr/lib/python3.8/logging/__init__.py\", line 1589, in _log\n",
      "    self.handle(record)\n",
      "  File \"/usr/lib/python3.8/logging/__init__.py\", line 1599, in handle\n",
      "    self.callHandlers(record)\n",
      "  File \"/usr/lib/python3.8/logging/__init__.py\", line 1661, in callHandlers\n",
      "    hdlr.handle(record)\n",
      "  File \"/usr/lib/python3.8/logging/__init__.py\", line 954, in handle\n",
      "    self.emit(record)\n",
      "  File \"/usr/local/lib/python3.8/dist-packages/pip/_internal/utils/logging.py\", line 179, in emit\n",
      "    self.handleError(record)\n",
      "Message: '[present-rich] %s'\n",
      "Arguments: (UpgradePrompt(old='22.2.2', new='24.0'),)\n"
     ]
    }
   ],
   "source": [
    "!pip install opencv-python\n",
    "\n",
    "import cv2\n",
    "import numpy as np\n",
    "\n",
    "\"\"\"\n",
    "|------|\n",
    "|      | height, y\n",
    "|      |\n",
    "|------|\n",
    " width, x\n",
    "\"\"\"\n",
    "\n",
    "def random_size(image, target_size=None):\n",
    "    height, width, _ = np.shape(image)\n",
    "    if target_size is None:\n",
    "        # for test\n",
    "        # target size is fixed\n",
    "        target_size = np.random.randint(*short_side_scale)\n",
    "    if height < width:\n",
    "        size_ratio = target_size / height\n",
    "    else:\n",
    "        size_ratio = target_size / width\n",
    "    resize_shape = (int(width * size_ratio), int(height * size_ratio))  # width and height in cv2 are opposite to np.shape()\n",
    "    return cv2.resize(image, resize_shape)\n",
    "\n",
    "def random_aspect(image):\n",
    "    height, width, _ = np.shape(image)\n",
    "    aspect_ratio = np.random.uniform(*aspect_ratio_scale)\n",
    "    if height < width:\n",
    "        resize_shape = (int(width * aspect_ratio), height)\n",
    "    else:\n",
    "        resize_shape = (width, int(height * aspect_ratio))\n",
    "    return cv2.resize(image, resize_shape)\n",
    "\n",
    "def random_crop(image):\n",
    "    height, width, _ = np.shape(image)\n",
    "    input_height, input_width, _ = input_shape\n",
    "    crop_x = np.random.randint(0, width - input_width)\n",
    "    crop_y = np.random.randint(0, height - input_height)\n",
    "    return image[crop_y: crop_y + input_height, crop_x: crop_x + input_width, :]\n",
    "\n",
    "def random_flip(image):\n",
    "    if np.random.rand() < 0.5:\n",
    "        image = cv2.flip(image, 1)\n",
    "    return image\n",
    "\n",
    "def random_hsv(image):\n",
    "    random_h = np.random.uniform(*hue_delta)\n",
    "    random_s = np.random.uniform(*saturation_scale)\n",
    "    random_v = np.random.uniform(*brightness_scale)\n",
    "\n",
    "    image_hsv = cv2.cvtColor(image, cv2.COLOR_BGR2HSV)\n",
    "    image_hsv[:, :, 0] = image_hsv[:, :, 0] + random_h % 360.0  # hue\n",
    "    image_hsv[:, :, 1] = np.minimum(image_hsv[:, :, 1] * random_s, 1.0)  # saturation\n",
    "    image_hsv[:, :, 2] = np.minimum(image_hsv[:, :, 2] * random_v, 255.0)  # brightness\n",
    "\n",
    "    return cv2.cvtColor(image_hsv, cv2.COLOR_HSV2BGR)\n",
    "\n",
    "def random_pca(image):\n",
    "    alpha = np.random.normal(0, pca_std, size=(3,))\n",
    "    offset = np.dot(eigvec * alpha, eigval)\n",
    "    image = image + offset\n",
    "    return np.maximum(np.minimum(image, 255.0), 0.0)\n",
    "\n",
    "def normalize(image):\n",
    "    for i in range(3):\n",
    "        image[..., i] = (image[..., i] - mean[i]) / std[i]\n",
    "    return image\n",
    "\n",
    "def center_crop(image):\n",
    "    height, width, _ = np.shape(image)\n",
    "    input_height, input_width, _ = input_shape\n",
    "    crop_x = (width - input_width) // 2\n",
    "    crop_y = (height - input_height) // 2\n",
    "    return image[crop_y: crop_y + input_height, crop_x: crop_x + input_width, :]\n",
    "\n",
    "def test_10_crop(image):\n",
    "    \"\"\"\n",
    "    Standard 10 crop for test.\n",
    "    Crop 4 corner and 1 center.\n",
    "    Then flip it.\n",
    "    \"\"\"\n",
    "    height, width, _ = np.shape(image)\n",
    "    input_height, input_width, _ = input_shape\n",
    "    center_crop_x = (width - input_width) // 2\n",
    "    center_crop_y = (height - input_height) // 2\n",
    "\n",
    "    images = []\n",
    "    images.append(image[:input_height, :input_width, :])  # left top\n",
    "    images.append(image[:input_height, -input_width:, :])  # right top\n",
    "    images.append(image[-input_height:, :input_width, :])  # left bottom\n",
    "    images.append(image[-input_height:, -input_width:, :])  # right bottom\n",
    "    images.append(image[center_crop_y: center_crop_y + input_height, center_crop_x: center_crop_x + input_width, :])\n",
    "\n",
    "    image = cv2.flip(image, 1)\n",
    "    images.append(image[:input_height, :input_width, :])  # left top\n",
    "    images.append(image[:input_height, -input_width:, :])  # right top\n",
    "    images.append(image[-input_height:, :input_width, :])  # left bottom\n",
    "    images.append(image[-input_height:, -input_width:, :])  # right bottom\n",
    "    images.append(image[center_crop_y: center_crop_y + input_height, center_crop_x: center_crop_x + input_width, :])\n",
    "\n",
    "    return np.array(images, dtype=np.float32)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "7bfd885b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "tf.enable_eager_execution()\n",
    "from tensorflow.keras.layers import Conv2D, BatchNormalization, Activation, MaxPool2D, Dropout, Flatten, Dense\n",
    "from tensorflow.keras import Model, models, Input, regularizers\n",
    "from tensorflow.keras.applications import VGG19, ResNet50, InceptionV3, DenseNet121, VGG16\n",
    "\n",
    "\n",
    "\n",
    "class VGG16Net(Model):\n",
    "    def __init__(self):\n",
    "        super(VGG16Net, self).__init__()\n",
    "        self.base_model = VGG16(weights='imagenet', include_top=False, pooling='max')\n",
    "        # self.base_model.trainable = False\n",
    "        for i in range(len(self.base_model.layers) - 2):  # print(len(model.layers))=23\n",
    "            self.base_model.layers[i].trainable = False\n",
    "        self.d1 = Dense(units=hash_bit, activation=tf.nn.sigmoid)\n",
    "        self.d2 = Dense(units=num_class, activation=tf.nn.softmax)\n",
    "\n",
    "    def call(self, x):\n",
    "        x = self.base_model(x)\n",
    "        hash_code = self.d1(x)\n",
    "        prediction = self.d2(hash_code)\n",
    "        return prediction, hash_code\n",
    "\n",
    "class VGG19Net(Model):\n",
    "    def __init__(self):\n",
    "        super(VGG19Net, self).__init__()\n",
    "        self.base_model = VGG19(weights='imagenet', include_top=False, pooling='max')\n",
    "        # self.base_model.trainable = False\n",
    "        #for i in range(len(self.base_model.layers)):\n",
    "        #    self.base_model.layers[i].trainable = False\n",
    "        self.d1 = Dense(units=hash_bit, activation=None)\n",
    "        self.bn1 = BatchNormalization()\n",
    "        self.ac1 = tf.keras.layers.Activation(\"sigmoid\")\n",
    "        self.d2 = Dense(units=num_class, activation=tf.nn.softmax)\n",
    "\n",
    "    def call(self, x):\n",
    "        x = self.base_model(x)\n",
    "        hash_code = self.d1(x)\n",
    "        hash_code = self.bn1(hash_code)\n",
    "        hash_code = self.ac1(hash_code)\n",
    "        prediction = self.d2(hash_code)\n",
    "        return prediction, hash_code    \n",
    "\n",
    "\n",
    "def Resnet_Model():\n",
    "    base_model = ResNet50(weights='imagenet', include_top=False, layers=tf.keras.layers, input_shape=(224, 224, 3))\n",
    "    print(len(base_model.layers))  # 175\n",
    "    #for layer in base_model.layers[:170]:\n",
    "    #    layer.trainable = False\n",
    "    #for layer in base_model.layers[170:]:\n",
    "    #    layer.trainable = True\n",
    "    x = base_model.output\n",
    "    x = tf.keras.layers.GlobalAveragePooling2D(name='average_pool')(x)\n",
    "    x = tf.keras.layers.Flatten(name='flatten')(x)\n",
    "    hash_value = tf.keras.layers.Dense(units=hash_bit, activation=None, kernel_regularizer=regularizers.l2(0.001))(x)\n",
    "    # hash_value = tf.keras.layers.Dense(units=hash_bit, activation=None)(x)\n",
    "    hash_value = tf.keras.layers.BatchNormalization()(hash_value)\n",
    "    hash_value = tf.keras.layers.Activation(\"sigmoid\")(hash_value)\n",
    "    prediction = tf.keras.layers.Dense(units=num_class, activation=\"softmax\",\n",
    "                                       kernel_regularizer=regularizers.l2(0.001))(hash_value)\n",
    "    # prediction = tf.keras.layers.Dense(units=num_class, activation=\"softmax\")(hash_value)\n",
    "    model = models.Model(inputs=base_model.input, outputs=[prediction, hash_value])\n",
    "    return model\n",
    "\n",
    "\n",
    "def DenseNet_Model():\n",
    "    base_model = DenseNet121(weights='imagenet', include_top=False, input_shape=(224, 224, 3))\n",
    "    base_model.trainable = False\n",
    "    print(len(base_model.layers))  # 427\n",
    "    for layer in base_model.layers[:415]:\n",
    "       layer.trainable = False\n",
    "    for layer in base_model.layers[415:]:\n",
    "       layer.trainable = True\n",
    "    x = base_model.output\n",
    "    x = tf.keras.layers.GlobalAveragePooling2D(name='average_pool')(x)\n",
    "    x = tf.keras.layers.Flatten(name='flatten')(x)\n",
    "    #x = tf.keras.layers.Dropout(0.5)(x)\n",
    "    hash_value = tf.keras.layers.Dense(units=hash_bit, activation=None)(x)\n",
    "    hash_value = tf.keras.layers.BatchNormalization()(hash_value)\n",
    "    hash_value = tf.keras.layers.Activation(\"sigmoid\")(hash_value)\n",
    "    prediction = tf.keras.layers.Dense(units=num_class, activation=\"softmax\")(hash_value)\n",
    "    model = models.Model(inputs=base_model.input, outputs=[prediction, hash_value])\n",
    "    return model\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "2f277e87",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import numpy as np\n",
    "import os\n",
    "import cv2\n",
    "tf.enable_eager_execution()\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def load_list(list_path, image_root_path):\n",
    "    images = []\n",
    "    labels = []\n",
    "    with open(list_path, 'r') as f:\n",
    "        for line in f:\n",
    "            lines = line.replace('\\n', '').split('\\t')\n",
    "            if len(lines) == 1:\n",
    "                lines = line.replace('\\n', '').split()\n",
    "            #print(lines)\n",
    "            # images.append(os.path.join(image_root_path, line[0]))\n",
    "            images.append(image_root_path + lines[0])\n",
    "            labels.append(int(lines[1]))\n",
    "    return images, labels\n",
    "\n",
    "\n",
    "def load_image(image_path, label, augment=False, crop_10=False):\n",
    "    \"\"\"\n",
    "    In training, it is highly recommended to set the augment to true.\n",
    "    In test, the standard 10-crop test [1] is provided for fair comparison.\n",
    "    [1] K. He, X. Zhang, S. Ren, and J. Sun. Deep residual learning for image recognition. In CVPR, 2016.\n",
    "    \"\"\"\n",
    "    image = cv2.imread(image_path.numpy().decode()).astype(np.float32)\n",
    "\n",
    "    if augment:\n",
    "        image = random_aspect(image)\n",
    "        image = random_size(image)\n",
    "        image = random_crop(image)\n",
    "        image = random_flip(image)\n",
    "        image = random_hsv(image)\n",
    "        image = random_pca(image)\n",
    "    else:\n",
    "        image = random_size(image, target_size=256)\n",
    "        if crop_10:\n",
    "            image = test_10_crop(image)\n",
    "        else:\n",
    "            image = center_crop(image)\n",
    "\n",
    "    image = normalize(image)\n",
    "\n",
    "    label_one_hot = np.zeros(num_class)\n",
    "    label_one_hot[label] = 1.0\n",
    "\n",
    "    return image, label_one_hot\n",
    "\n",
    "\n",
    "def train_iterator(list_path=imagenet_train_path):\n",
    "    # list_path图片名称，train_data_path图片前缀地址\n",
    "    \n",
    "\n",
    "    images, labels = load_list(list_path, train_data_path)\n",
    "    #print(images, labels)\n",
    "    #with open(\"image-labels.txt\", 'w') as f:\n",
    "    #        f.write(\"images\"+str(images))\n",
    "    #        f.write(\"\\n\")\n",
    "    #        f.write(\"labels\"+str(labels))\n",
    "    dataset = tf.data.Dataset.from_tensor_slices((images, labels))\n",
    "    dataset = dataset.shuffle(len(images))\n",
    "    dataset = dataset.repeat()\n",
    "    dataset = dataset.map(\n",
    "        lambda x, y: tf.py_function(load_image, inp=[x, y, True, False], Tout=[tf.float32, tf.float32]),\n",
    "        num_parallel_calls=tf.data.experimental.AUTOTUNE).prefetch(tf.data.experimental.AUTOTUNE)\n",
    "    dataset = dataset.batch(batch_size)\n",
    "    it = dataset.__iter__()\n",
    "    return it\n",
    "\n",
    "\n",
    "def test_iterator(list_path=imagenet_test_path):\n",
    "    images, labels = load_list(list_path, test_data_path)\n",
    "    \n",
    "    print(\"images:\",len(images),\"labels:\",len(labels))\n",
    "    dataset = tf.data.Dataset.from_tensor_slices((images, labels))\n",
    "    dataset = dataset.map(\n",
    "        lambda x, y: tf.py_function(load_image, inp=[x, y, False, False], Tout=[tf.float32, tf.float32]),\n",
    "        num_parallel_calls=tf.data.experimental.AUTOTUNE).prefetch(tf.data.experimental.AUTOTUNE)\n",
    "    dataset = dataset.batch(batch_size)\n",
    "    it = dataset.__iter__()\n",
    "    return it\n",
    "\n",
    "\n",
    "def test_10_crop_iterator(list_path=imagenet_test_path):\n",
    "    images, labels = load_list(list_path, test_data_path)\n",
    "    dataset = tf.data.Dataset.from_tensor_slices((images, labels))\n",
    "    dataset = dataset.map(\n",
    "        lambda x, y: tf.py_function(load_image, inp=[x, y, False, True], Tout=[tf.float32, tf.float32]),\n",
    "        num_parallel_calls=tf.data.experimental.AUTOTUNE)\n",
    "    it = dataset.__iter__()\n",
    "    return it\n",
    "\n",
    "\n",
    "# if __name__ == '__main__':\n",
    "#     # Annotate the 'normalize' function for visualization\n",
    "#     # it = train_iterator()\n",
    "#     it = test_iterator('/content/test.txt')\n",
    "#     images, labels = it.next()\n",
    "#     # print(np.shape(images), np.shape(labels))\n",
    "#     for i in range(10):\n",
    "#         print(np.where(labels[i].numpy() != 0))\n",
    "#         cv2_imshow( images[i].numpy().astype(np.uint8))\n",
    "#         cv2.waitKey(0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "61cebb20",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import numpy as np\n",
    "import os\n",
    "import cv2\n",
    "tf.enable_eager_execution()\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def load_list(list_path, image_root_path):\n",
    "    images = []\n",
    "    labels = []\n",
    "    with open(list_path, 'r') as f:\n",
    "        for line in f:\n",
    "            lines = line.replace('\\n', '').split('\\t')\n",
    "            if len(lines) == 1:\n",
    "                lines = line.replace('\\n', '').split()\n",
    "            #print(lines)\n",
    "            # images.append(os.path.join(image_root_path, line[0]))\n",
    "            images.append(image_root_path + lines[0])\n",
    "            labels.append(int(lines[1]))\n",
    "    return images, labels\n",
    "\n",
    "\n",
    "def load_image(image_path, label, augment=False, crop_10=False):\n",
    "    \"\"\"\n",
    "    In training, it is highly recommended to set the augment to true.\n",
    "    In test, the standard 10-crop test [1] is provided for fair comparison.\n",
    "    [1] K. He, X. Zhang, S. Ren, and J. Sun. Deep residual learning for image recognition. In CVPR, 2016.\n",
    "    \"\"\"\n",
    "    image = cv2.imread(image_path.numpy().decode()).astype(np.float32)\n",
    "\n",
    "    if augment:\n",
    "        image = random_aspect(image)\n",
    "        image = random_size(image)\n",
    "        image = random_crop(image)\n",
    "        image = random_flip(image)\n",
    "        image = random_hsv(image)\n",
    "        image = random_pca(image)\n",
    "    else:\n",
    "        image = random_size(image, target_size=256)\n",
    "        if crop_10:\n",
    "            image = test_10_crop(image)\n",
    "        else:\n",
    "            image = center_crop(image)\n",
    "\n",
    "    image = normalize(image)\n",
    "\n",
    "    label_one_hot = np.zeros(num_class)\n",
    "    label_one_hot[label] = 1.0\n",
    "\n",
    "    return image, label_one_hot\n",
    "\n",
    "\n",
    "def train_iterator(list_path=imagenet_train_path):\n",
    "    # list_path图片名称，train_data_path图片前缀地址\n",
    "    \n",
    "\n",
    "    images, labels = load_list(list_path, train_data_path)\n",
    "    #print(images, labels)\n",
    "    #with open(\"image-labels.txt\", 'w') as f:\n",
    "    #        f.write(\"images\"+str(images))\n",
    "    #        f.write(\"\\n\")\n",
    "    #        f.write(\"labels\"+str(labels))\n",
    "    dataset = tf.data.Dataset.from_tensor_slices((images, labels))\n",
    "    dataset = dataset.shuffle(len(images))\n",
    "    dataset = dataset.repeat()\n",
    "    dataset = dataset.map(\n",
    "        lambda x, y: tf.py_function(load_image, inp=[x, y, True, False], Tout=[tf.float32, tf.float32]),\n",
    "        num_parallel_calls=tf.data.experimental.AUTOTUNE).prefetch(tf.data.experimental.AUTOTUNE)\n",
    "    dataset = dataset.batch(batch_size)\n",
    "    it = dataset.__iter__()\n",
    "    return it\n",
    "\n",
    "\n",
    "def test_iterator(list_path=imagenet_test_path):\n",
    "    images, labels = load_list(list_path, test_data_path)\n",
    "    \n",
    "    print(\"images:\",len(images),\"labels:\",len(labels))\n",
    "    dataset = tf.data.Dataset.from_tensor_slices((images, labels))\n",
    "    dataset = dataset.map(\n",
    "        lambda x, y: tf.py_function(load_image, inp=[x, y, False, False], Tout=[tf.float32, tf.float32]),\n",
    "        num_parallel_calls=tf.data.experimental.AUTOTUNE).prefetch(tf.data.experimental.AUTOTUNE)\n",
    "    dataset = dataset.batch(batch_size)\n",
    "    it = dataset.__iter__()\n",
    "    return it\n",
    "\n",
    "\n",
    "def test_10_crop_iterator(list_path=imagenet_test_path):\n",
    "    images, labels = load_list(list_path, test_data_path)\n",
    "    dataset = tf.data.Dataset.from_tensor_slices((images, labels))\n",
    "    dataset = dataset.map(\n",
    "        lambda x, y: tf.py_function(load_image, inp=[x, y, False, True], Tout=[tf.float32, tf.float32]),\n",
    "        num_parallel_calls=tf.data.experimental.AUTOTUNE)\n",
    "    it = dataset.__iter__()\n",
    "    return it\n",
    "\n",
    "\n",
    "# if __name__ == '__main__':\n",
    "#     # Annotate the 'normalize' function for visualization\n",
    "#     # it = train_iterator()\n",
    "#     it = test_iterator('/content/test.txt')\n",
    "#     images, labels = it.next()\n",
    "#     # print(np.shape(images), np.shape(labels))\n",
    "#     for i in range(10):\n",
    "#         print(np.where(labels[i].numpy() != 0))\n",
    "#         cv2_imshow( images[i].numpy().astype(np.uint8))\n",
    "#         cv2.waitKey(0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58157717",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61a542fa",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-05-02 19:30:43.221653: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcuda.so.1\n",
      "2024-05-02 19:30:43.323234: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1666] Found device 0 with properties: \n",
      "name: NVIDIA A100 80GB PCIe major: 8 minor: 0 memoryClockRate(GHz): 1.41\n",
      "pciBusID: 0000:4b:00.0\n",
      "2024-05-02 19:30:43.323317: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcudart.so.11.0\n",
      "2024-05-02 19:30:43.370872: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcublas.so.11\n",
      "2024-05-02 19:30:43.422102: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcufft.so.10\n",
      "2024-05-02 19:30:43.422540: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcurand.so.10\n",
      "2024-05-02 19:30:43.439434: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcusolver.so.11\n",
      "2024-05-02 19:30:43.444794: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcusparse.so.11\n",
      "2024-05-02 19:30:43.445039: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcudnn.so.8\n",
      "2024-05-02 19:30:43.447542: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1794] Adding visible gpu devices: 0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[PhysicalDevice(name='/physical_device:GPU:0', device_type='GPU')]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-05-02 19:30:43.507207: I tensorflow/core/platform/profile_utils/cpu_utils.cc:109] CPU Frequency: 2100000000 Hz\n",
      "2024-05-02 19:30:43.510459: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x4f38390 initialized for platform Host (this does not guarantee that XLA will be used). Devices:\n",
      "2024-05-02 19:30:43.510502: I tensorflow/compiler/xla/service/service.cc:176]   StreamExecutor device (0): Host, Default Version\n",
      "2024-05-02 19:30:43.699148: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x2596590 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:\n",
      "2024-05-02 19:30:43.699210: I tensorflow/compiler/xla/service/service.cc:176]   StreamExecutor device (0): NVIDIA A100 80GB PCIe, Compute Capability 8.0\n",
      "2024-05-02 19:30:43.700744: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1666] Found device 0 with properties: \n",
      "name: NVIDIA A100 80GB PCIe major: 8 minor: 0 memoryClockRate(GHz): 1.41\n",
      "pciBusID: 0000:4b:00.0\n",
      "2024-05-02 19:30:43.700796: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcudart.so.11.0\n",
      "2024-05-02 19:30:43.700847: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcublas.so.11\n",
      "2024-05-02 19:30:43.700861: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcufft.so.10\n",
      "2024-05-02 19:30:43.700875: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcurand.so.10\n",
      "2024-05-02 19:30:43.700889: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcusolver.so.11\n",
      "2024-05-02 19:30:43.700901: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcusparse.so.11\n",
      "2024-05-02 19:30:43.700915: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcudnn.so.8\n",
      "2024-05-02 19:30:43.703175: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1794] Adding visible gpu devices: 0\n",
      "2024-05-02 19:30:43.703206: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcudart.so.11.0\n",
      "2024-05-02 19:30:43.926414: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1206] Device interconnect StreamExecutor with strength 1 edge matrix:\n",
      "2024-05-02 19:30:43.926470: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1212]      0 \n",
      "2024-05-02 19:30:43.926480: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1225] 0:   N \n",
      "2024-05-02 19:30:43.929777: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1351] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 78028 MB memory) -> physical GPU (device: 0, name: NVIDIA A100 80GB PCIe, pci bus id: 0000:4b:00.0, compute capability: 8.0)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading data from https://github.com/fchollet/deep-learning-models/releases/download/v0.1/vgg19_weights_tf_dim_ordering_tf_kernels_notop.h5\n",
      "80142336/80134624 [==============================] - 12s 0us/step\n",
      "Model: \"vg_g19net\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "vgg19 (Model)                (None, 512)               20024384  \n",
      "_________________________________________________________________\n",
      "dense (Dense)                multiple                  262656    \n",
      "_________________________________________________________________\n",
      "batch_normalization (BatchNo multiple                  2048      \n",
      "_________________________________________________________________\n",
      "activation (Activation)      multiple                  0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              multiple                  246240    \n",
      "=================================================================\n",
      "Total params: 20,535,328\n",
      "Trainable params: 20,534,304\n",
      "Non-trainable params: 1,024\n",
      "_________________________________________________________________\n",
      "WARNING:tensorflow:TensorFlow will not use sklearn by default. This improves performance in some cases. To enable sklearn export the environment variable  TF_ALLOW_IOLIBS=1.\n",
      "WARNING:tensorflow:TensorFlow will not use Dask by default. This improves performance in some cases. To enable Dask export the environment variable  TF_ALLOW_IOLIBS=1.\n",
      "WARNING:tensorflow:TensorFlow will not use Pandas by default. This improves performance in some cases. To enable Pandas export the environment variable  TF_ALLOW_IOLIBS=1.\n",
      "0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|                                                                                                | 0/1236 [00:00<?, ?it/s]2024-05-02 19:31:02.747467: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcublas.so.11\n",
      "2024-05-02 19:31:03.640574: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcudnn.so.8\n",
      "100%|█████████████████████████████████████████████████████████████████████████████████████| 1236/1236 [12:41<00:00,  1.62it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ce_loss:5.837192, l2_loss:0.132265,hash_loss:0.201769 ,accuracy:0.009096\n",
      "images: 39539 labels: 39539\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████████████████████████████████████| 309/309 [01:50<00:00,  2.79it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test, loss:5.480041, accuracy:0.013226\n",
      "1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████████████████████████████████| 1236/1236 [09:06<00:00,  2.26it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ce_loss:4.675946, l2_loss:0.175849,hash_loss:0.188960 ,accuracy:0.077436\n",
      "images: 39539 labels: 39539\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████████████████████████████████████| 309/309 [01:26<00:00,  3.58it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test, loss:4.110666, accuracy:0.126456\n",
      "2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████████████████████████████████| 1236/1236 [09:03<00:00,  2.27it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ce_loss:3.470265, l2_loss:0.248136,hash_loss:0.171370 ,accuracy:0.249488\n",
      "images: 39539 labels: 39539\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████████████████████████████████████| 309/309 [01:31<00:00,  3.39it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test, loss:3.155443, accuracy:0.287192\n",
      "3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████████████████████████████████| 1236/1236 [09:00<00:00,  2.29it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ce_loss:2.575575, l2_loss:0.325241,hash_loss:0.157705 ,accuracy:0.425623\n",
      "images: 39539 labels: 39539\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████████████████████████████████████| 309/309 [01:39<00:00,  3.10it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test, loss:2.174461, accuracy:0.489879\n",
      "4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████████████████████████████████| 1236/1236 [08:58<00:00,  2.30it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ce_loss:2.010597, l2_loss:0.391304,hash_loss:0.148169 ,accuracy:0.544644\n",
      "images: 39539 labels: 39539\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████████████████████████████████████| 309/309 [01:44<00:00,  2.96it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test, loss:1.721267, accuracy:0.601976\n",
      "5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████████████████████████████████| 1236/1236 [08:57<00:00,  2.30it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ce_loss:1.614294, l2_loss:0.444394,hash_loss:0.140984 ,accuracy:0.630790\n",
      "images: 39539 labels: 39539\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████████████████████████████████████| 309/309 [01:55<00:00,  2.68it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test, loss:1.507703, accuracy:0.641736\n",
      "6\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████████████████████████████████| 1236/1236 [08:59<00:00,  2.29it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ce_loss:1.360523, l2_loss:0.485955,hash_loss:0.135283 ,accuracy:0.685781\n",
      "images: 39539 labels: 39539\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████████████████████████████████████| 309/309 [02:03<00:00,  2.51it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test, loss:1.252685, accuracy:0.702699\n",
      "7\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████████████████████████████████| 1236/1236 [09:37<00:00,  2.14it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ce_loss:1.167945, l2_loss:0.517593,hash_loss:0.130651 ,accuracy:0.728819\n",
      "images: 39539 labels: 39539\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████████████████████████████████████| 309/309 [01:57<00:00,  2.63it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test, loss:1.234885, accuracy:0.706202\n",
      "8\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████████████████████████████████| 1236/1236 [09:11<00:00,  2.24it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ce_loss:1.031608, l2_loss:0.541668,hash_loss:0.126696 ,accuracy:0.759867\n",
      "images: 39539 labels: 39539\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████████████████████████████████████| 309/309 [02:08<00:00,  2.41it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test, loss:0.887906, accuracy:0.788677\n",
      "9\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████████████████████████████████| 1236/1236 [08:59<00:00,  2.29it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ce_loss:0.929115, l2_loss:0.559790,hash_loss:0.123382 ,accuracy:0.783696\n",
      "images: 39539 labels: 39539\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████████████████████████████████████| 309/309 [02:00<00:00,  2.55it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test, loss:0.791433, accuracy:0.817474\n",
      "10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████████████████████████████████| 1236/1236 [09:31<00:00,  2.16it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ce_loss:0.841644, l2_loss:0.572443,hash_loss:0.120470 ,accuracy:0.803683\n",
      "images: 39539 labels: 39539\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████████████████████████████████████| 309/309 [02:07<00:00,  2.41it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test, loss:0.831291, accuracy:0.799568\n",
      "11\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████████████████████████████████| 1236/1236 [10:57<00:00,  1.88it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ce_loss:0.779764, l2_loss:0.581847,hash_loss:0.117907 ,accuracy:0.817620\n",
      "images: 39539 labels: 39539\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████████████████████████████████████| 309/309 [01:39<00:00,  3.11it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test, loss:0.722997, accuracy:0.828972\n",
      "12\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████████████████████████████████| 1236/1236 [08:57<00:00,  2.30it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ce_loss:0.710940, l2_loss:0.587209,hash_loss:0.115549 ,accuracy:0.834225\n",
      "images: 39539 labels: 39539\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████████████████████████████████████| 309/309 [02:18<00:00,  2.23it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test, loss:0.702785, accuracy:0.834591\n",
      "13\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████████████████████████████████| 1236/1236 [11:42<00:00,  1.76it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ce_loss:0.665606, l2_loss:0.590058,hash_loss:0.113532 ,accuracy:0.844894\n",
      "images: 39539 labels: 39539\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████████████████████████████████████| 309/309 [02:34<00:00,  2.00it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test, loss:0.647998, accuracy:0.846578\n",
      "14\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████████████████████████████████| 1236/1236 [08:06<00:00,  2.54it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ce_loss:0.619021, l2_loss:0.591229,hash_loss:0.111641 ,accuracy:0.855696\n",
      "images: 39539 labels: 39539\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████████████████████████████████████| 309/309 [02:12<00:00,  2.34it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test, loss:0.623297, accuracy:0.849295\n",
      "15\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████████████████████████████████| 1236/1236 [11:44<00:00,  1.75it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ce_loss:0.585952, l2_loss:0.590788,hash_loss:0.109950 ,accuracy:0.863623\n",
      "images: 39539 labels: 39539\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████████████████████████████████████| 309/309 [01:40<00:00,  3.08it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test, loss:0.559156, accuracy:0.867884\n",
      "16\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████████████████████████████████| 1236/1236 [08:44<00:00,  2.36it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ce_loss:0.551589, l2_loss:0.588899,hash_loss:0.108422 ,accuracy:0.871966\n",
      "images: 39539 labels: 39539\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████████████████████████████████████| 309/309 [02:20<00:00,  2.20it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test, loss:0.541342, accuracy:0.873777\n",
      "17\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████████████████████████████████| 1236/1236 [09:41<00:00,  2.13it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ce_loss:0.511034, l2_loss:0.585118,hash_loss:0.106974 ,accuracy:0.881435\n",
      "images: 39539 labels: 39539\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████████████████████████████████████| 309/309 [02:07<00:00,  2.42it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test, loss:0.478770, accuracy:0.888111\n",
      "18\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████████████████████████████████| 1236/1236 [09:35<00:00,  2.15it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ce_loss:0.484593, l2_loss:0.580455,hash_loss:0.105703 ,accuracy:0.888071\n",
      "images: 39539 labels: 39539\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████████████████████████████████████| 309/309 [02:09<00:00,  2.39it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test, loss:0.507891, accuracy:0.879624\n",
      "19\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████████████████████████████████| 1236/1236 [09:59<00:00,  2.06it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ce_loss:0.459764, l2_loss:0.575177,hash_loss:0.104551 ,accuracy:0.893855\n",
      "images: 39539 labels: 39539\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████████████████████████████████████| 309/309 [02:14<00:00,  2.29it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test, loss:0.511431, accuracy:0.876435\n",
      "20\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████████████████████████████████| 1236/1236 [09:37<00:00,  2.14it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ce_loss:0.432424, l2_loss:0.569029,hash_loss:0.103392 ,accuracy:0.901528\n",
      "images: 39539 labels: 39539\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████████████████████████████████████| 309/309 [02:12<00:00,  2.34it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test, loss:0.471754, accuracy:0.890487\n",
      "21\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████████████████████████████████| 1236/1236 [09:51<00:00,  2.09it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ce_loss:0.411408, l2_loss:0.562706,hash_loss:0.102435 ,accuracy:0.906098\n",
      "images: 39539 labels: 39539\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████████████████████████████████████| 309/309 [02:15<00:00,  2.28it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test, loss:0.449264, accuracy:0.893791\n",
      "22\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████████████████████████████████| 1236/1236 [09:32<00:00,  2.16it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ce_loss:0.386327, l2_loss:0.555542,hash_loss:0.101511 ,accuracy:0.912470\n",
      "images: 39539 labels: 39539\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████████████████████████████████████| 309/309 [02:04<00:00,  2.48it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test, loss:0.391502, accuracy:0.905795\n",
      "23\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████████████████████████████████| 1236/1236 [09:32<00:00,  2.16it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ce_loss:0.366578, l2_loss:0.547946,hash_loss:0.100668 ,accuracy:0.916875\n",
      "images: 39539 labels: 39539\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████████████████████████████████████| 309/309 [02:02<00:00,  2.53it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test, loss:0.395576, accuracy:0.910104\n",
      "24\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████████████████████████████████| 1236/1236 [09:23<00:00,  2.19it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ce_loss:0.342713, l2_loss:0.539734,hash_loss:0.099865 ,accuracy:0.923544\n",
      "images: 39539 labels: 39539\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████████████████████████████████████| 309/309 [02:05<00:00,  2.46it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test, loss:0.367536, accuracy:0.914042\n",
      "25\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████████████████████████████████| 1236/1236 [09:44<00:00,  2.11it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ce_loss:0.325121, l2_loss:0.531800,hash_loss:0.099165 ,accuracy:0.927551\n",
      "images: 39539 labels: 39539\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 50%|███████████████████████████████████████████▎                                           | 154/309 [01:04<01:59,  1.30it/s]"
     ]
    }
   ],
   "source": [
    "#!/usr/bin/python?\n",
    "# -*- coding: utf-8 -*-import tensorflow as tf\n",
    "import numpy as np\n",
    "import pickle as p\n",
    "from tqdm import tqdm\n",
    "import os\n",
    "import cv2\n",
    "from tensorflow.keras import models, optimizers, Sequential\n",
    "from tensorflow.keras.layers import Input\n",
    "\n",
    "# from load_data import train_iterator, test_iterator\n",
    "import tensorflow as tf\n",
    "\n",
    "#import datetime\n",
    "#current_time = datetime.datetime.now().strftime(\"%Y%m%d-%H%M%S\")\n",
    "# log_dir = 'logs/Resnet/' + \"cos\"\n",
    "# summary_writer = tf.summary.create_file_writer(log_dir) \n",
    "plot_train_loss=[]\n",
    "plot_test_loss=[]\n",
    "plot_train_acc=[]\n",
    "plot_test_acc=[]\n",
    "class CosineDecayWithWarmUP(tf.keras.experimental.CosineDecay):\n",
    "    def __init__(self, initial_learning_rate, decay_steps, alpha=0.0, warm_up_step=0, name=None):\n",
    "        self.warm_up_step = warm_up_step\n",
    "        super(CosineDecayWithWarmUP, self).__init__(initial_learning_rate=initial_learning_rate,\n",
    "                                                    decay_steps=decay_steps,\n",
    "                                                    alpha=alpha,\n",
    "                                                    name=name)\n",
    "\n",
    "def hash_loss_fn(hash_input):\n",
    "    loss1 = -1 * tf.reduce_mean(tf.square(hash_input - 0.5)) + 0.25  # 最大值为0.25\n",
    "    loss2 = tf.reduce_mean(tf.square(tf.reduce_mean(hash_input, axis=1) - 0.5))\n",
    "    return loss1 + loss2\n",
    "\n",
    "\n",
    "def cross_entropy(y_true, y_pred):\n",
    "    cross_entropy = tf.keras.losses.categorical_crossentropy(y_true, y_pred)\n",
    "    return tf.reduce_mean(cross_entropy)\n",
    "\n",
    "\n",
    "def l2_loss(model, weights=weight_decay):\n",
    "    variable_list = []\n",
    "    for v in model.trainable_variables:\n",
    "        if 'kernel' in v.name:\n",
    "            variable_list.append(tf.nn.l2_loss(v))\n",
    "    return tf.add_n(variable_list) * weights\n",
    "\n",
    "\n",
    "def accuracy(y_true, y_pred):\n",
    "    correct_num = tf.equal(tf.argmax(y_true, -1), tf.argmax(y_pred, -1))\n",
    "    accuracy = tf.reduce_mean(tf.cast(correct_num, dtype=tf.float32))\n",
    "    return accuracy\n",
    "\n",
    "\n",
    "@tf.function\n",
    "def train_step(model, optimizer, x, y):\n",
    "    with tf.GradientTape() as tape:\n",
    "        prediction, hash_value = model(x, training=True)  # 修改\n",
    "        hash_loss = hash_loss_fn(hash_value)  # 修改\n",
    "        ce = cross_entropy(y, prediction)\n",
    "        l2 = l2_loss(model)\n",
    "        loss = ce + l2 + hash_loss  # 修改\n",
    "        gradients = tape.gradient(loss, model.trainable_variables)\n",
    "    optimizer.apply_gradients(zip(gradients, model.trainable_variables))\n",
    "\n",
    "    return ce, hash_loss, prediction\n",
    "\n",
    "\n",
    "@tf.function\n",
    "def test_step(model, x, y):\n",
    "    prediction, _ = model(x, training=False)  # 修改\n",
    "    ce = cross_entropy(y, prediction)\n",
    "    return ce, prediction\n",
    "\n",
    "\n",
    "def train(model, optimizer, train_iterator, log_file):\n",
    "    sum_loss = 0\n",
    "    sum_hash_loss = 0\n",
    "    sum_accuracy = 0\n",
    "\n",
    "    for i in tqdm(range(iterations_per_epoch)):\n",
    "        x, y = train_iterator.next()\n",
    "        loss, hash_loss, prediction = train_step(model, optimizer, x, y)\n",
    "        sum_hash_loss += hash_loss\n",
    "        sum_loss += loss\n",
    "        sum_accuracy += accuracy(y, prediction)\n",
    "\n",
    "    print('ce_loss:%f, l2_loss:%f,hash_loss:%f ,accuracy:%f' %\n",
    "          (sum_loss / iterations_per_epoch, l2_loss(model), sum_hash_loss / iterations_per_epoch,\n",
    "           sum_accuracy / iterations_per_epoch))\n",
    "    log_file.write('train: ce loss: {:.4f}, l2 loss: {:.4f}, hash_loss: {:.4f}, accuracy: {:.4f}\\n'.format(sum_loss / iterations_per_epoch, l2_loss(model), \n",
    "    sum_hash_loss / iterations_per_epoch, sum_accuracy / iterations_per_epoch ))\n",
    "    plot_train_loss.append(sum_loss/iterations_per_epoch)\n",
    "    plot_train_loss.append(sum_accuracy/iterations_per_epoch)\n",
    "\n",
    "\n",
    "def test(model, test_iter, log_file):\n",
    "    sum_loss = 0\n",
    "    sum_accuracy = 0\n",
    "    test_data_iterator = test_iterator()\n",
    "\n",
    "    for i in tqdm(range(test_iter)):\n",
    "        x, y = test_data_iterator.next()\n",
    "        x = tf.cast(x, tf.float32)  # 将测试集中的图像编码成float32\n",
    "        loss, prediction = test_step(model, x, y)\n",
    "        sum_loss += loss\n",
    "        sum_accuracy += accuracy(y, prediction)\n",
    "    print('test, loss:%f, accuracy:%f' %\n",
    "          (sum_loss / test_iterations, sum_accuracy / test_iterations))\n",
    "    log_file.write('test: loss: {:.4f}, accuracy: {:.4f}\\n'.format(sum_loss / test_iterations, sum_accuracy / test_iterations))\n",
    "    plot_test_loss.append(sum_loss/test_iterations)\n",
    "    plot_test_loss.append(sum_accuracy/test_iterations)\n",
    "    return sum_accuracy / test_iterations\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    # gpu config\n",
    "    gpus = tf.config.experimental.list_physical_devices(device_type='GPU')\n",
    "    for gpu in gpus:\n",
    "        tf.config.experimental.set_memory_growth(gpu, True)\n",
    "    import os\n",
    "\n",
    "    os.environ['CUDA_VISIBLE_DEVICES'] = '1'\n",
    "    physical_devices = tf.config.experimental.list_physical_devices(device_type='GPU')\n",
    "    print(physical_devices)\n",
    "\n",
    "    tf.config.experimental.set_memory_growth(device=physical_devices[0], enable=True)\n",
    "\n",
    "    # show\n",
    "    model = VGG19Net()\n",
    "    model.build((None,224,224,3))\n",
    "    model.summary()\n",
    "\n",
    "    # train-cos\n",
    "    learning_rate_schedules = CosineDecayWithWarmUP(initial_learning_rate=initial_learning_rate,\n",
    "                                                decay_steps=epoch_num * iterations_per_epoch - warm_iterations,\n",
    "                                                alpha=minimum_learning_rate,\n",
    "                                                warm_up_step=warm_iterations)\n",
    "    optimizer = optimizers.SGD(learning_rate=learning_rate_schedules, momentum=0.9)\n",
    "    checkpoint = tf.train.Checkpoint(myAwesomeModel=model)\n",
    "    \n",
    "    # train\n",
    "    #learning_rate_schedules = optimizers.schedules.PiecewiseConstantDecay(c.boundaries, c.learning_rate)\n",
    "    #optimizer = optimizers.SGD(learning_rate=learning_rate_schedules, momentum=0.9, nesterov=True)\n",
    "    \n",
    "    # optimizer = optimizers.Adam(learning_rate=learning_rate_schedules)\n",
    "    # optimizer = optimizers.Adam()\n",
    "    checkpoint = tf.train.Checkpoint(myAwesomeModel=model)\n",
    "\n",
    "    train_data_iterator = train_iterator()\n",
    "    for epoch in range(epoch_num):\n",
    "        print(epoch)\n",
    "        with open(log_file, 'a') as f:\n",
    "            f.write('epoch:{}\\n'.format(epoch))\n",
    "            train(model, optimizer, train_data_iterator, f)\n",
    "            test_acc = test(model, test_iterations, f)\n",
    "        if test_acc > 0.5:\n",
    "            name = \"./checkpoint/vgg_new/\" + str(np.round(test_acc.numpy(), 3)) + '/model.ckpt'\n",
    "            if not os.path.exists(\"./checkpoint/vgg_new/\" + str(np.round(test_acc.numpy(), 3))):\n",
    "                os.makedirs(\"./checkpoint/vgg_new/\" + str(np.round(test_acc.numpy(), 3)))\n",
    "            checkpoint.save(name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83bb2667",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "pochs = range(epoch_num)\n",
    "print(len(plot_train_loss))\n",
    "kkk=0\n",
    "plot_ac_train_loss=[]\n",
    "plot_ac_test_loss=[]\n",
    "for i in plot_train_loss:\n",
    "    if kkk==0:\n",
    "        plot_ac_train_loss.append(i)\n",
    "        kkk=1\n",
    "    else:\n",
    "        kkk=0\n",
    "\n",
    "kkk=0\n",
    "for i in plot_test_loss:\n",
    "    if kkk==0:\n",
    "        plot_ac_test_loss.append(i)\n",
    "        kkk=1\n",
    "    else:\n",
    "        kkk=0\n",
    "plt.plot(pochs, plot_ac_train_loss, label='Train Loss')\n",
    "\n",
    "plt.plot(pochs, plot_ac_test_loss, label='Test Loss')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Loss')\n",
    "plt.title('Training and Testing Loss')\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "199d461b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "pochs = range(epoch_num)\n",
    "\n",
    "kkk=1\n",
    "plot_ac_train_acc=[]\n",
    "plot_ac_test_acc=[]\n",
    "for i in plot_train_loss:\n",
    "    if kkk==0:\n",
    "        plot_ac_train_acc.append(i)\n",
    "        kkk=1\n",
    "    else:\n",
    "        kkk=0\n",
    "\n",
    "kkk=1\n",
    "for i in plot_test_loss:\n",
    "    if kkk==0:\n",
    "        plot_ac_test_acc.append(i)\n",
    "        kkk=1\n",
    "    else:\n",
    "        kkk=0\n",
    "print(len(plot_test_acc))    \n",
    "plt.plot(pochs, plot_ac_train_acc, label='Train Accuracy')\n",
    "plt.plot(pochs, plot_ac_test_acc, label='Test Accuracy')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Loss')\n",
    "plt.title('Training and Testing Accuracy')\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "353d16cb",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d894a130",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
